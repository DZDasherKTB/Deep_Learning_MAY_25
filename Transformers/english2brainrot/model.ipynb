{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104dbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a22499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model,num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    assert d_model % num_heads == 0\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_heads = num_heads\n",
    "    self.d_k = d_model // num_heads\n",
    "\n",
    "    self.W_q = nn.Linear(d_model, d_model)\n",
    "    self.W_k = nn.Linear(d_model, d_model)\n",
    "    self.W_v = nn.Linear(d_model, d_model)\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def scaled_dot_product_attention(self, Q, K, V, mask = None):\n",
    "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "      attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    attn_probs = torch.softmax(attn_scores, dim = -1)\n",
    "\n",
    "    output = torch.matmul(attn_probs, V)\n",
    "    return output\n",
    "  \n",
    "  def split_heads(self, x):\n",
    "    batch_size, seq_length, embed_dim = x.size()\n",
    "    x = x.view(batch_size, seq_length, self.num_heads, self.d_k)\n",
    "    return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "  def combine_heads(self, x):\n",
    "    batch_size, _, seq_length, d_k = x.size()\n",
    "    return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "  def forward(self, Q,K,V, mask = None):\n",
    "    Q = self.split_heads(self.W_q(Q))\n",
    "    K = self.split_heads(self.W_q(K))\n",
    "    V = self.split_heads(self.W_q(V))\n",
    "\n",
    "\n",
    "    attn_output = self.scaled_dot_product_attention(Q,K,V,mask)\n",
    "    \n",
    "    output = self.W_o(self.combine_heads(attn_output))\n",
    "    return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "  def __init__(self, d_model, d_ff):\n",
    "    super(PositionWiseFeedForward,self).__init__()\n",
    "    self.fc1 = nn.Linear(d_model, d_ff)\n",
    "    self.fc2 = nn.Linear(d_ff,d_model)\n",
    "    self.relu = nn.ReLU()\n",
    "  def forward(self, x):\n",
    "    return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self,d_model,max_seq_length):\n",
    "    super(PositionalEncoding,self).__init__()\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "    position = torch.arange(0,max_seq_length,dtype = torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0,d_model,2).float()* -(math.log(10000.0)/d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_model,d_ff)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  def forward(self, x, mask):\n",
    "    attn_output = self.self_attn(x, x, x, mask)\n",
    "    x = self.norm1(x +self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm2(x +self.dropout(ff_output))\n",
    "    return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self,d_model, num_heads, d_ff, dropout):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.self_attn = MultiHeadAttention(d_model,num_heads)\n",
    "    self.cross_attn = MultiHeadAttention(d_model,num_heads)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.norm3 = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "    attn_output = self.self_attn(x,x,x, tgt_mask)\n",
    "    x = self.norm1(x + self.dropout(attn_output))\n",
    "    attn_output = self.cross_attn(x,enc_output,enc_output,src_mask)\n",
    "    x = self.norm2(x + self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm3(x + self.dropout(ff_output))\n",
    "    return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads,num_layers,d_ff,max_seq_length,dropout):\n",
    "    super(Transformer,self).__init__()\n",
    "    self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "    self.decoder_embedding = nn.Embedding(tgt_vocab_size,d_model)\n",
    "    self.positional_encoding = PositionalEncoding(d_model,max_seq_length)\n",
    "    self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "    self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "    self.fc = nn.Linear(d_model,tgt_vocab_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def generate_mask(self, src, tgt):\n",
    "    device = src.device \n",
    "\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(device)  # ensure device\n",
    "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(device)\n",
    "\n",
    "    seq_length = tgt.size(1)\n",
    "\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
    "\n",
    "    tgt_mask = tgt_mask & nopeak_mask\n",
    "\n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "  def forward(self,src,tgt):\n",
    "    src_mask,tgt_mask = self.generate_mask(src,tgt)\n",
    "    src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "    tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "    enc_output = src_embedded\n",
    "    for enc_layer in self.encoder_layers:\n",
    "      enc_output = enc_layer(enc_output,src_mask)\n",
    "    \n",
    "    dec_output = tgt_embedded\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "    output = self.fc(dec_output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222121cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4171c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5663 training and 505 test sentence pairs.\n",
      "Train loader batches: 80, Val loader batches: 9, Test loader batches: 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import spacy\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Load SpaCy English tokenizer\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Brainrot tokenizer: simple whitespace tokenizer (customize as needed)\n",
    "def tokenize_brainrot(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Load JSONL dataset (list of (source, target) tuples)\n",
    "def load_jsonl_dataset(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            data.append((obj['source'], obj['target']))\n",
    "    return data\n",
    "\n",
    "# Special tokens\n",
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "# Build vocabulary from data, given tokenizer and which index in tuple to use (0=source,1=target)\n",
    "def build_vocab_manual(data, index, tokenizer, specials=SPECIAL_TOKENS, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for pair in data:\n",
    "        tokens = tokenizer(pair[index])\n",
    "        counter.update(tokens)\n",
    "    itos = list(specials)\n",
    "    itos += [tok for tok, freq in counter.items() if freq >= min_freq and tok not in specials]\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "# Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, src_stoi, tgt_stoi, src_tokenizer, tgt_tokenizer):\n",
    "        self.data = data\n",
    "        self.src_stoi = src_stoi\n",
    "        self.tgt_stoi = tgt_stoi\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.PAD_IDX = src_stoi[\"<pad>\"]\n",
    "        self.UNK_IDX = src_stoi[\"<unk>\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence, tgt_sentence = self.data[idx]\n",
    "        src_tokens = [\"<sos>\"] + self.src_tokenizer(src_sentence) + [\"<eos>\"]\n",
    "        tgt_tokens = [\"<sos>\"] + self.tgt_tokenizer(tgt_sentence) + [\"<eos>\"]\n",
    "\n",
    "        src_ids = torch.tensor([self.src_stoi.get(tok, self.UNK_IDX) for tok in src_tokens], dtype=torch.long)\n",
    "        tgt_ids = torch.tensor([self.tgt_stoi.get(tok, self.UNK_IDX) for tok in tgt_tokens], dtype=torch.long)\n",
    "\n",
    "        return src_ids, tgt_ids\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_padded = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_padded = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "# Main: Load data, build vocabs, create datasets and dataloaders\n",
    "\n",
    "# Load train and test data\n",
    "train_data = load_jsonl_dataset(\"data/train.jsonl\")\n",
    "test_data = load_jsonl_dataset(\"data/test.jsonl\")\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training and {len(test_data)} test sentence pairs.\")\n",
    "\n",
    "# Build vocabularies on training data only\n",
    "src_stoi, src_itos = build_vocab_manual(train_data, 0, tokenize_en)\n",
    "tgt_stoi, tgt_itos = build_vocab_manual(train_data, 1, tokenize_brainrot)\n",
    "\n",
    "PAD_IDX = src_stoi[\"<pad>\"]\n",
    "UNK_IDX = src_stoi[\"<unk>\"]\n",
    "SOS_IDX = src_stoi[\"<sos>\"]\n",
    "EOS_IDX = src_stoi[\"<eos>\"]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TranslationDataset(train_data, src_stoi, tgt_stoi, tokenize_en, tokenize_brainrot)\n",
    "test_dataset = TranslationDataset(test_data, src_stoi, tgt_stoi, tokenize_en, tokenize_brainrot)\n",
    "\n",
    "# Optional: split train into train/val (e.g., 90/10)\n",
    "train_len = int(0.9 * len(train_dataset))\n",
    "val_len = len(train_dataset) - train_len\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_len, val_len])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train loader batches: {len(train_loader)}, Val loader batches: {len(val_loader)}, Test loader batches: {len(test_loader)}\")\n",
    "SRC_VOCAB_SIZE = len(src_stoi)  # Number of tokens in source vocabulary (English)\n",
    "TGT_VOCAB_SIZE = len(tgt_stoi)  # Number of tokens in target vocabulary (German)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90bed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Allocated GPU memory: 0.12 GB\n",
      "Reserved GPU memory:  0.19 GB\n",
      "Max Allocated GPU:    0.50 GB\n",
      "Max Reserved GPU:     1.29 GB\n",
      "Epoch 01 | Train Loss: 8.8102 | Val Loss: 8.3510\n",
      "Epoch 02 | Train Loss: 7.9957 | Val Loss: 7.7510\n",
      "Epoch 03 | Train Loss: 7.4728 | Val Loss: 7.4050\n",
      "Epoch 04 | Train Loss: 7.1834 | Val Loss: 7.2478\n",
      "Epoch 05 | Train Loss: 7.0322 | Val Loss: 7.1720\n",
      "Epoch 06 | Train Loss: 6.9326 | Val Loss: 7.1122\n",
      "Epoch 07 | Train Loss: 6.8443 | Val Loss: 7.0590\n",
      "Epoch 08 | Train Loss: 6.7652 | Val Loss: 7.0145\n",
      "Epoch 09 | Train Loss: 6.6857 | Val Loss: 6.9774\n",
      "Epoch 10 | Train Loss: 6.6116 | Val Loss: 6.9442\n",
      "Epoch 11 | Train Loss: 6.5476 | Val Loss: 6.9210\n",
      "Epoch 12 | Train Loss: 6.4849 | Val Loss: 6.8920\n",
      "Epoch 13 | Train Loss: 6.4289 | Val Loss: 6.8741\n",
      "Epoch 14 | Train Loss: 6.3753 | Val Loss: 6.8514\n",
      "Epoch 15 | Train Loss: 6.3247 | Val Loss: 6.8498\n",
      "Epoch 16 | Train Loss: 6.2782 | Val Loss: 6.8307\n",
      "Epoch 17 | Train Loss: 6.2343 | Val Loss: 6.8169\n",
      "Epoch 18 | Train Loss: 6.1919 | Val Loss: 6.8156\n",
      "Epoch 19 | Train Loss: 6.1520 | Val Loss: 6.8047\n",
      "Epoch 20 | Train Loss: 6.1155 | Val Loss: 6.8008\n",
      "Epoch 21 | Train Loss: 6.0811 | Val Loss: 6.7943\n",
      "Epoch 22 | Train Loss: 6.0420 | Val Loss: 6.8008\n",
      "Epoch 23 | Train Loss: 6.0116 | Val Loss: 6.7716\n",
      "Epoch 24 | Train Loss: 5.9793 | Val Loss: 6.7925\n",
      "Epoch 25 | Train Loss: 5.9482 | Val Loss: 6.7831\n",
      "Epoch 26 | Train Loss: 5.9141 | Val Loss: 6.7733\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, tgt_output)\n\u001b[0;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 64\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     67\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming your Transformer class is implemented somewhere, e.g.:\n",
    "# from model import Transformer\n",
    "# or define your Transformer model accordingly\n",
    "\n",
    "model = Transformer(\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    d_ff=512,\n",
    "    max_seq_length=100,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# IMPORTANT: use tgt padding idx for ignore_index since targets are compared in loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_stoi[\"<pad>\"])\n",
    "\n",
    "print(f\"Allocated GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved GPU memory:  {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "print(f\"Max Allocated GPU:    {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Max Reserved GPU:     {torch.cuda.max_memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "num_epochs = 50\n",
    "teacher_forcing_ratio = 0.5  # e.g. 50% teacher forcing\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # Prepare input and output for decoder\n",
    "        tgt_input = tgt[:, :-1]   # all except last token\n",
    "        tgt_output = tgt[:, 1:]   # all except first token (shifted by 1)\n",
    "\n",
    "        batch_size, seq_len = tgt_input.size()\n",
    "\n",
    "        # Forward pass with ground truth input to get initial predictions\n",
    "        preds = model(src, tgt_input)  # shape: [batch, seq_len, vocab_size]\n",
    "        preds_tokens = preds.argmax(dim=-1)  # greedy predictions\n",
    "\n",
    "        # Decide for each token whether to use ground truth or predicted token\n",
    "        mask = torch.rand(batch_size, seq_len, device=device) < teacher_forcing_ratio\n",
    "\n",
    "        # Mix ground truth and predicted tokens\n",
    "        mixed_tgt_input = torch.where(mask, tgt_input, preds_tokens)\n",
    "\n",
    "        # Forward pass again with mixed input\n",
    "        preds = model(src, mixed_tgt_input)\n",
    "        preds = preds.reshape(-1, preds.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "        loss = criterion(preds, tgt_output)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            preds = model(src, tgt_input)\n",
    "            preds = preds.reshape(-1, preds.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "            val_loss = criterion(preds, tgt_output)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ce4987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to model_epoch_27.pth\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Saved checkpoint to {path}\")\n",
    "save_checkpoint(model, optimizer, epoch, path=\"model_epoch_{}.pth\".format(epoch+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f60e9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started working\n",
      "Epoch 1 Train Loss: 5.8664 | Val Loss: 6.7592\n",
      "Epoch 2 Train Loss: 5.8459 | Val Loss: 6.7751\n",
      "Epoch 3 Train Loss: 5.8305 | Val Loss: 6.7559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m batch_size, seq_len \u001b[38;5;241m=\u001b[39m tgt_input\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass once with ground-truth for initial prediction\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, seq_len, vocab_size]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m preds_tokens \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# greedy predictions: [batch, seq_len]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Decide for each token whether to use ground truth or predicted token\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 140\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m    138\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m tgt_embedded\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[1;32m--> 140\u001b[0m   dec_output \u001b[38;5;241m=\u001b[39m \u001b[43mdec_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(dec_output)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 96\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[1;34m(self, x, enc_output, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, enc_output, src_mask, tgt_mask):\n\u001b[1;32m---> 96\u001b[0m   attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output))\n\u001b[0;32m     98\u001b[0m   attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(x,enc_output,enc_output,src_mask)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[0;32m     38\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_q(K))\n\u001b[0;32m     39\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_q(V))\n\u001b[1;32m---> 42\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_o(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_heads(attn_output))\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m, in \u001b[0;36mMultiHeadAttention.scaled_dot_product_attention\u001b[1;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscaled_dot_product_attention\u001b[39m(\u001b[38;5;28mself\u001b[39m, Q, K, V, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 16\u001b[0m   attn_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m attn_scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005, betas=(0.9, 0.98), eps=1e-9)\n",
    "print(\"started working\")\n",
    "for epoch in range(5):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    teacher_forcing_ratio = 0.5  # e.g. 70% teacher forcing\n",
    "\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]   # ground-truth input tokens\n",
    "        tgt_output = tgt[:, 1:]   # expected output tokens\n",
    "        \n",
    "        batch_size, seq_len = tgt_input.shape\n",
    "        \n",
    "        # Forward pass once with ground-truth for initial prediction\n",
    "        preds = model(src, tgt_input)  # [batch, seq_len, vocab_size]\n",
    "        preds_tokens = preds.argmax(dim=-1)  # greedy predictions: [batch, seq_len]\n",
    "        \n",
    "        # Decide for each token whether to use ground truth or predicted token\n",
    "        mask = torch.rand(batch_size, seq_len, device=device) < teacher_forcing_ratio\n",
    "        \n",
    "        # Construct mixed tgt_input:\n",
    "        mixed_tgt_input = torch.where(mask, tgt_input, preds_tokens)\n",
    "        \n",
    "        # Forward pass with mixed input\n",
    "        preds = model(src, mixed_tgt_input)\n",
    "        preds = preds.reshape(-1, preds.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        \n",
    "        loss = criterion(preds, tgt_output)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            preds = model(src, tgt_input)\n",
    "            preds = preds.reshape(-1, preds.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "            val_loss = criterion(preds, tgt_output)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2acd37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to model_epoch_4.pth\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Saved checkpoint to {path}\")\n",
    "save_checkpoint(model, optimizer, epoch, path=\"model_epoch_{}.pth\".format(epoch+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af1db73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started working\n",
      "Epoch 1 Train Loss: 5.8140 | Val Loss: 6.7566\n",
      "Epoch 2 Train Loss: 5.8100 | Val Loss: 6.7548\n",
      "Epoch 3 Train Loss: 5.8094 | Val Loss: 6.7515\n",
      "Epoch 4 Train Loss: 5.8069 | Val Loss: 6.7467\n",
      "Epoch 5 Train Loss: 5.8060 | Val Loss: 6.7470\n",
      "Epoch 6 Train Loss: 5.8035 | Val Loss: 6.7409\n",
      "Epoch 7 Train Loss: 5.8037 | Val Loss: 6.7405\n",
      "Epoch 8 Train Loss: 5.7991 | Val Loss: 6.7363\n",
      "Epoch 9 Train Loss: 5.7997 | Val Loss: 6.7347\n",
      "Epoch 10 Train Loss: 5.7969 | Val Loss: 6.7274\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000005, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-5)\n",
    "print(\"started working\")\n",
    "for epoch in range(10):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    teacher_forcing_ratio = 0.5  # e.g. 70% teacher forcing\n",
    "\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]   # ground-truth input tokens\n",
    "        tgt_output = tgt[:, 1:]   # expected output tokens\n",
    "        \n",
    "        batch_size, seq_len = tgt_input.shape\n",
    "        \n",
    "        # Forward pass once with ground-truth for initial prediction\n",
    "        preds = model(src, tgt_input)  # [batch, seq_len, vocab_size]\n",
    "        preds_tokens = preds.argmax(dim=-1)  # greedy predictions: [batch, seq_len]\n",
    "        \n",
    "        # Decide for each token whether to use ground truth or predicted token\n",
    "        mask = torch.rand(batch_size, seq_len, device=device) < teacher_forcing_ratio\n",
    "        \n",
    "        # Construct mixed tgt_input:\n",
    "        mixed_tgt_input = torch.where(mask, tgt_input, preds_tokens)\n",
    "        \n",
    "        # Forward pass with mixed input\n",
    "        preds = model(src, mixed_tgt_input)\n",
    "        preds = preds.reshape(-1, preds.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        \n",
    "        loss = criterion(preds, tgt_output)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            preds = model(src, tgt_input)\n",
    "            preds = preds.reshape(-1, preds.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "            val_loss = criterion(preds, tgt_output)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a938637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to model_epoch_10.pth\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Saved checkpoint to {path}\")\n",
    "save_checkpoint(model, optimizer, epoch, path=\"model_epoch_{}.pth\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad2488c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started working\n",
      "Epoch 1 Train Loss: 5.6848 | Val Loss: 6.4316\n",
      "Epoch 2 Train Loss: 5.6810 | Val Loss: 6.4278\n",
      "Epoch 3 Train Loss: 5.6817 | Val Loss: 6.4275\n",
      "Epoch 4 Train Loss: 5.6803 | Val Loss: 6.4262\n",
      "Epoch 5 Train Loss: 5.6776 | Val Loss: 6.4245\n",
      "Epoch 6 Train Loss: 5.6731 | Val Loss: 6.4186\n",
      "Epoch 7 Train Loss: 5.6744 | Val Loss: 6.4146\n",
      "Epoch 8 Train Loss: 5.6706 | Val Loss: 6.4177\n",
      "Epoch 9 Train Loss: 5.6699 | Val Loss: 6.4147\n",
      "Epoch 10 Train Loss: 5.6687 | Val Loss: 6.4126\n",
      "Epoch 11 Train Loss: 5.6706 | Val Loss: 6.4102\n",
      "Epoch 12 Train Loss: 5.6699 | Val Loss: 6.4074\n",
      "Epoch 13 Train Loss: 5.6666 | Val Loss: 6.4057\n",
      "Epoch 14 Train Loss: 5.6642 | Val Loss: 6.4040\n",
      "Epoch 15 Train Loss: 5.6641 | Val Loss: 6.4002\n",
      "Epoch 16 Train Loss: 5.6630 | Val Loss: 6.3978\n",
      "Epoch 17 Train Loss: 5.6586 | Val Loss: 6.3990\n",
      "Epoch 18 Train Loss: 5.6625 | Val Loss: 6.3921\n",
      "Epoch 19 Train Loss: 5.6629 | Val Loss: 6.3947\n",
      "Epoch 20 Train Loss: 5.6587 | Val Loss: 6.3901\n",
      "Epoch 21 Train Loss: 5.6560 | Val Loss: 6.3889\n",
      "Epoch 22 Train Loss: 5.6581 | Val Loss: 6.3862\n",
      "Epoch 23 Train Loss: 5.6556 | Val Loss: 6.3843\n",
      "Epoch 24 Train Loss: 5.6512 | Val Loss: 6.3812\n",
      "Epoch 25 Train Loss: 5.6547 | Val Loss: 6.3787\n",
      "Epoch 26 Train Loss: 5.6521 | Val Loss: 6.3775\n",
      "Epoch 27 Train Loss: 5.6492 | Val Loss: 6.3723\n",
      "Epoch 28 Train Loss: 5.6463 | Val Loss: 6.3734\n",
      "Epoch 29 Train Loss: 5.6459 | Val Loss: 6.3716\n",
      "Epoch 30 Train Loss: 5.6433 | Val Loss: 6.3665\n",
      "Epoch 31 Train Loss: 5.6443 | Val Loss: 6.3673\n",
      "Epoch 32 Train Loss: 5.6417 | Val Loss: 6.3640\n",
      "Epoch 33 Train Loss: 5.6417 | Val Loss: 6.3610\n",
      "Epoch 34 Train Loss: 5.6410 | Val Loss: 6.3567\n",
      "Epoch 35 Train Loss: 5.6405 | Val Loss: 6.3574\n",
      "Epoch 36 Train Loss: 5.6385 | Val Loss: 6.3567\n",
      "Epoch 37 Train Loss: 5.6388 | Val Loss: 6.3526\n",
      "Epoch 38 Train Loss: 5.6371 | Val Loss: 6.3498\n",
      "Epoch 39 Train Loss: 5.6353 | Val Loss: 6.3491\n",
      "Epoch 40 Train Loss: 5.6342 | Val Loss: 6.3462\n",
      "Epoch 41 Train Loss: 5.6323 | Val Loss: 6.3434\n",
      "Epoch 42 Train Loss: 5.6339 | Val Loss: 6.3409\n",
      "Epoch 43 Train Loss: 5.6291 | Val Loss: 6.3409\n",
      "Epoch 44 Train Loss: 5.6315 | Val Loss: 6.3369\n",
      "Epoch 45 Train Loss: 5.6278 | Val Loss: 6.3339\n",
      "Epoch 46 Train Loss: 5.6299 | Val Loss: 6.3327\n",
      "Epoch 47 Train Loss: 5.6220 | Val Loss: 6.3320\n",
      "Epoch 48 Train Loss: 5.6271 | Val Loss: 6.3308\n",
      "Epoch 49 Train Loss: 5.6247 | Val Loss: 6.3276\n",
      "Epoch 50 Train Loss: 5.6211 | Val Loss: 6.3278\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000005, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-5)\n",
    "print(\"started working\")\n",
    "for epoch in range(50):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    teacher_forcing_ratio = 0.3  # e.g. 70% teacher forcing\n",
    "\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]   # ground-truth input tokens\n",
    "        tgt_output = tgt[:, 1:]   # expected output tokens\n",
    "        \n",
    "        batch_size, seq_len = tgt_input.shape\n",
    "        \n",
    "        # Forward pass once with ground-truth for initial prediction\n",
    "        preds = model(src, tgt_input)  # [batch, seq_len, vocab_size]\n",
    "        preds_tokens = preds.argmax(dim=-1)  # greedy predictions: [batch, seq_len]\n",
    "        \n",
    "        # Decide for each token whether to use ground truth or predicted token\n",
    "        mask = torch.rand(batch_size, seq_len, device=device) < teacher_forcing_ratio\n",
    "        \n",
    "        # Construct mixed tgt_input:\n",
    "        mixed_tgt_input = torch.where(mask, tgt_input, preds_tokens)\n",
    "        \n",
    "        # Forward pass with mixed input\n",
    "        preds = model(src, mixed_tgt_input)\n",
    "        preds = preds.reshape(-1, preds.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        \n",
    "        loss = criterion(preds, tgt_output)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            preds = model(src, tgt_input)\n",
    "            preds = preds.reshape(-1, preds.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "            val_loss = criterion(preds, tgt_output)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d403aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to model_epoch_50.pth\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Saved checkpoint to {path}\")\n",
    "save_checkpoint(model, optimizer, epoch, path=\"model_epoch_{}.pth\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2529ab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Saved tokenizer and datasets to save_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "save_dict = {\n",
    "    \"src_stoi\": src_stoi,\n",
    "    \"tgt_stoi\": tgt_stoi,\n",
    "    \"src_itos\": src_itos if 'src_itos' in locals() else None,\n",
    "    \"tgt_itos\": tgt_itos if 'tgt_itos' in locals() else None,\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"val_dataset\": val_dataset\n",
    "}\n",
    "\n",
    "with open(\"save_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(save_dict, f)\n",
    "\n",
    "print(\"✔️ Saved tokenizer and datasets to save_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63404156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from model-1.pth, starting at epoch 27\n"
     ]
    }
   ],
   "source": [
    "def load_checkpoint(model, optimizer, path=\"checkpoint.pth\", device='cuda'):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint.get(\"epoch\", 0)\n",
    "    print(f\"Loaded checkpoint from {path}, starting at epoch {epoch+1}\")\n",
    "    return epoch\n",
    "start_epoch = load_checkpoint(model, optimizer, path=\"model-1.pth\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6303041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: <sos> my my my my rn 😩 gotta gotta gotta gotta gotta fr, fr, 🙏 🙏 🙏 🙏 🙏 <eos>\n"
     ]
    }
   ],
   "source": [
    "def evaluate_with_beam_search(model, src_sentence, src_stoi, tgt_stoi, tgt_itos,\n",
    "                              beam_width=8, max_len=50, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    sos_token_id = tgt_stoi[\"<sos>\"]\n",
    "    eos_token_id = tgt_stoi[\"<eos>\"]\n",
    "\n",
    "    src_tensor = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Initialize beam with sos token\n",
    "    beams = [(torch.tensor([[sos_token_id]], device=device), 0.0)]  # (sequence, score)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "                if seq[0, -1].item() == eos_token_id:\n",
    "                    # Already ended beam — keep it as-is\n",
    "                    new_beams.append((seq, score))\n",
    "                    continue\n",
    "\n",
    "                output = model(src_tensor, seq)  # (1, seq_len, vocab_size)\n",
    "                logits = output[0, -1, :]        # last token logits\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "                # Get top beam_width predictions\n",
    "                topk_log_probs, topk_ids = torch.topk(log_probs, beam_width)\n",
    "\n",
    "                for log_prob, token_id in zip(topk_log_probs, topk_ids):\n",
    "                    new_seq = torch.cat([seq, token_id.view(1, 1)], dim=1)\n",
    "                    new_score = score + log_prob.item()\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "\n",
    "            # Keep only top `beam_width` beams\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "            # If all beams have ended, stop early\n",
    "            if all(seq[0, -1].item() == eos_token_id for seq, _ in beams):\n",
    "                break\n",
    "\n",
    "    # Choose the best-scoring sequence\n",
    "    best_seq = beams[0][0][0].tolist()  # shape (seq_len,)\n",
    "    predicted_tokens = [tgt_itos[i] if i < len(tgt_itos) else \"<unk>\" for i in best_seq]\n",
    "\n",
    "    return predicted_tokens\n",
    "\n",
    "def evaluate(model, src_sentence, src_stoi, tgt_stoi, tgt_itos, max_len=50, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and convert source sentence using your tokenizer, here assumed pre-tokenized list of ints\n",
    "    src_tensor = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(device)  # (1, src_len)\n",
    "    \n",
    "    sos_token_id = tgt_stoi[\"<sos>\"]\n",
    "    eos_token_id = tgt_stoi[\"<eos>\"]\n",
    "    \n",
    "    tgt_tensor = torch.tensor([[sos_token_id]], dtype=torch.long).to(device)  # (1,1)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, tgt_tensor)  # (1, tgt_len, vocab_size)\n",
    "        \n",
    "        next_token_logits = output[0, -1, :]  # logits for last token\n",
    "        next_token_id = torch.argmax(next_token_logits).item()\n",
    "        \n",
    "        tgt_tensor = torch.cat([tgt_tensor, torch.tensor([[next_token_id]], device=device)], dim=1)\n",
    "        \n",
    "        if next_token_id == eos_token_id:\n",
    "            break\n",
    "    \n",
    "    predicted_ids = tgt_tensor[0].tolist()\n",
    "    predicted_tokens = [tgt_itos[i] if i < len(tgt_itos) else \"<unk>\" for i in predicted_ids]\n",
    "    \n",
    "    return predicted_tokens\n",
    "\n",
    "def evaluate_top_p(\n",
    "    model, src_sentence, src_stoi, tgt_stoi, tgt_itos,\n",
    "    max_len=50, device='cuda', temperature=0.5, top_p=0.8\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert src_sentence (list of token ids) to tensor and add batch dimension\n",
    "    src_tensor = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(device)  # shape: (1, src_len)\n",
    "\n",
    "    sos_token_id = tgt_stoi[\"<sos>\"]\n",
    "    eos_token_id = tgt_stoi[\"<eos>\"]\n",
    "\n",
    "    # Start target sequence with <sos>, shape (1,1)\n",
    "    tgt_tensor = torch.tensor([[sos_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            # Pass src and current target input to the model\n",
    "            output = model(src_tensor, tgt_tensor)  # shape: (1, tgt_len, vocab_size)\n",
    "\n",
    "        logits = output[0, -1, :] / temperature  # last token logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Top-p (nucleus) sampling\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "        sorted_probs[cumulative_probs > top_p] = 0\n",
    "        total_prob = sorted_probs.sum()\n",
    "\n",
    "        if total_prob == 0:\n",
    "            next_token_id = torch.argmax(probs).item()\n",
    "        else:\n",
    "            sorted_probs /= total_prob\n",
    "            sampled_index = torch.multinomial(sorted_probs, 1).item()\n",
    "            next_token_id = sorted_indices[sampled_index].item()\n",
    "\n",
    "        tgt_tensor = torch.cat([tgt_tensor, torch.tensor([[next_token_id]], device=device)], dim=1)\n",
    "\n",
    "        if next_token_id == eos_token_id:\n",
    "            break\n",
    "\n",
    "    predicted_ids = tgt_tensor[0].tolist()\n",
    "    predicted_tokens = [tgt_itos[i] if i < len(tgt_itos) else \"<unk>\" for i in predicted_ids]\n",
    "\n",
    "    return predicted_tokens\n",
    "\n",
    "# Use your actual tokenizer here instead of split()\n",
    "src_text = \"I need to finish my homework.\"\n",
    "src_tokens = tokenize_en(src_text)  # <-- your actual tokenizer function\n",
    "src_ids = [src_stoi.get(tok, src_stoi[\"<unk>\"]) for tok in src_tokens]\n",
    "\n",
    "result = evaluate(model, src_ids, src_stoi, tgt_stoi, tgt_itos, device=device)\n",
    "print(\"Generated:\", \" \".join(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cc42eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Input: Hello, how are you?\n",
      "    Output: <sos> she got the a a a a a a a a a a a <eos>\n",
      "\n",
      "[2] Input: Despite the rain, they went hiking in the mountains.\n",
      "    Output: <sos> nah that the the the the the the the the the the the the the the the the the <eos>\n",
      "\n",
      "[3] Input: Thank you.\n",
      "    Output: <sos> her just a a a a a a a a a a a a <eos>\n",
      "\n",
      "[4] Input: The philosopher questioned the fabric of reality.\n",
      "    Output: <sos> nah that that a a a a a a a a a the the the <eos>\n",
      "\n",
      "[5] Input: I really like you.\n",
      "    Output: <sos> my my is a a a <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Despite the rain, they went hiking in the mountains.\",\n",
    "    \"Thank you.\",\n",
    "    \"The philosopher questioned the fabric of reality.\",\n",
    "    \"I really like you.\"\n",
    "]\n",
    "\n",
    "for i, src_text in enumerate(test_sentences, 1):\n",
    "    src_tokens = tokenize_en(src_text)\n",
    "    src_ids = [src_stoi.get(tok, src_stoi[\"<unk>\"]) for tok in src_tokens]\n",
    "    \n",
    "    result = evaluate(model, src_ids, src_stoi, tgt_stoi, tgt_itos, device=device)\n",
    "    print(f\"[{i}] Input: {src_text}\")\n",
    "    print(f\"    Output: {' '.join(result)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95563124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Input: Hello, how are you?\n",
      "    Output: <sos> hallo , wie bist du ? ? <eos>\n",
      "\n",
      "[2] Input: Despite the rain, they went hiking in the mountains.\n",
      "    Output: <sos> der regen gingen sie in in den bergen . . <eos>\n",
      "\n",
      "[3] Input: Thank you.\n",
      "    Output: <sos> danke danke ! danke danke danke danke ! <eos>\n",
      "\n",
      "[4] Input: The philosopher questioned the fabric of reality.\n",
      "    Output: <sos> sie haben die die beste der der wirklichkeit . <eos>\n",
      "\n",
      "[5] Input: I really like you.\n",
      "    Output: <sos> ich mag dich wirklich gern . <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, src_text in enumerate(test_sentences, 1):\n",
    "    src_tokens = tokenize_en(src_text)\n",
    "    src_ids = [src_stoi.get(tok, src_stoi[\"<unk>\"]) for tok in src_tokens]\n",
    "    \n",
    "    result = evaluate_with_beam_search(model, src_ids, src_stoi, tgt_stoi, tgt_itos, device=device)\n",
    "    print(f\"[{i}] Input: {src_text}\")\n",
    "    print(f\"    Output: {' '.join(result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48ac2122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Input: Hello, how are you?\n",
      "    Output: <sos> she got the a a a <eos>\n",
      "\n",
      "[2] Input: Despite the rain, they went hiking in the mountains.\n",
      "    Output: <sos> nah that that the and a a and and needed, for the and the the the the <eos>\n",
      "\n",
      "[3] Input: Thank you.\n",
      "    Output: <sos> nah her u u but in a a a a my of u <eos>\n",
      "\n",
      "[4] Input: The philosopher questioned the fabric of reality.\n",
      "    Output: <sos> nah the the a a a to see ✨ the my <eos>\n",
      "\n",
      "[5] Input: I really like you.\n",
      "    Output: <sos> i'm my a a a <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, src_text in enumerate(test_sentences, 1):\n",
    "    src_tokens = tokenize_en(src_text)\n",
    "    src_ids = [src_stoi.get(tok, src_stoi[\"<unk>\"]) for tok in src_tokens]\n",
    "    \n",
    "    result = evaluate_top_p(model, src_ids, src_stoi, tgt_stoi, tgt_itos, device=device, top_p=0.9, temperature= 0.5)\n",
    "    print(f\"[{i}] Input: {src_text}\")\n",
    "    print(f\"    Output: {' '.join(result)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c724d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
