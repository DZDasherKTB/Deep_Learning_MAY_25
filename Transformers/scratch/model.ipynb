{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104dbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a22499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model,num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    assert d_model % num_heads == 0\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_heads = num_heads\n",
    "    self.d_k = d_model // num_heads\n",
    "\n",
    "    self.W_q = nn.Linear(d_model, d_model)\n",
    "    self.W_k = nn.Linear(d_model, d_model)\n",
    "    self.W_v = nn.Linear(d_model, d_model)\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def scaled_dot_product_attention(self, Q, K, V, mask = None):\n",
    "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "      attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    attn_probs = torch.softmax(attn_scores, dim = -1)\n",
    "\n",
    "    output = torch.matmul(attn_probs, V)\n",
    "    return output\n",
    "  \n",
    "  def split_heads(self, x):\n",
    "    batch_size, seq_length, embed_dim = x.size()\n",
    "    x = x.view(batch_size, seq_length, self.num_heads, self.d_k)\n",
    "    return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "  def combine_heads(self, x):\n",
    "    batch_size, _, seq_length, d_k = x.size()\n",
    "    return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "  def forward(self, Q,K,V, mask = None):\n",
    "    Q = self.split_heads(self.W_q(Q))\n",
    "    K = self.split_heads(self.W_q(K))\n",
    "    V = self.split_heads(self.W_q(V))\n",
    "\n",
    "\n",
    "    attn_output = self.scaled_dot_product_attention(Q,K,V,mask)\n",
    "    \n",
    "    output = self.W_o(self.combine_heads(attn_output))\n",
    "    return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "  def __init__(self, d_model, d_ff):\n",
    "    super(PositionWiseFeedForward,self).__init__()\n",
    "    self.fc1 = nn.Linear(d_model, d_ff)\n",
    "    self.fc2 = nn.Linear(d_ff,d_model)\n",
    "    self.relu = nn.ReLU()\n",
    "  def forward(self, x):\n",
    "    return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self,d_model,max_seq_length):\n",
    "    super(PositionalEncoding,self).__init__()\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "    position = torch.arange(0,max_seq_length,dtype = torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0,d_model,2).float()* -(math.log(10000.0)/d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_model,d_ff)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  def forward(self, x, mask):\n",
    "    attn_output = self.self_attn(x, x, x, mask)\n",
    "    x = self.norm1(x +self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm2(x +self.dropout(ff_output))\n",
    "    return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self,d_model, num_heads, d_ff, dropout):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.self_attn = MultiHeadAttention(d_model,num_heads)\n",
    "    self.cross_attn = MultiHeadAttention(d_model,num_heads)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.norm3 = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "    attn_output = self.self_attn(x,x,x, tgt_mask)\n",
    "    x = self.norm1(x + self.dropout(attn_output))\n",
    "    attn_output = self.cross_attn(x,enc_output,enc_output,src_mask)\n",
    "    x = self.norm2(x + self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm3(x + self.dropout(ff_output))\n",
    "    return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads,num_layers,d_ff,max_seq_length,dropout):\n",
    "    super(Transformer,self).__init__()\n",
    "    self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "    self.decoder_embedding = nn.Embedding(tgt_vocab_size,d_model)\n",
    "    self.positional_encoding = PositionalEncoding(d_model,max_seq_length)\n",
    "    self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "    self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "    self.fc = nn.Linear(d_model,tgt_vocab_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def generate_mask(self, src, tgt):\n",
    "    device = src.device \n",
    "\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(device)  # ensure device\n",
    "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(device)\n",
    "\n",
    "    seq_length = tgt.size(1)\n",
    "\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
    "\n",
    "    tgt_mask = tgt_mask & nopeak_mask\n",
    "\n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "  def forward(self,src,tgt):\n",
    "    src_mask,tgt_mask = self.generate_mask(src,tgt)\n",
    "    src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "    tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "    enc_output = src_embedded\n",
    "    for enc_layer in self.encoder_layers:\n",
    "      enc_output = enc_layer(enc_output,src_mask)\n",
    "    \n",
    "    dec_output = tgt_embedded\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "    output = self.fc(dec_output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222121cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4171c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 sentence pairs\n",
      "cuda\n",
      "Epoch 1 Loss: 7.7248\n",
      "Epoch 2 Loss: 7.3152\n",
      "Epoch 3 Loss: 6.9438\n",
      "Epoch 4 Loss: 6.6477\n",
      "Epoch 5 Loss: 6.4008\n",
      "Epoch 6 Loss: 6.1973\n",
      "Epoch 7 Loss: 6.0205\n",
      "Epoch 8 Loss: 5.8626\n",
      "Epoch 9 Loss: 5.7197\n",
      "Epoch 10 Loss: 5.6018\n",
      "Epoch 11 Loss: 5.5011\n",
      "Epoch 12 Loss: 5.4093\n",
      "Epoch 13 Loss: 5.3368\n",
      "Epoch 14 Loss: 5.2678\n",
      "Epoch 15 Loss: 5.1963\n",
      "Epoch 16 Loss: 5.1406\n",
      "Epoch 17 Loss: 5.0858\n",
      "Epoch 18 Loss: 5.0334\n",
      "Epoch 19 Loss: 4.9875\n",
      "Epoch 20 Loss: 4.9402\n",
      "Epoch 21 Loss: 4.8886\n",
      "Epoch 22 Loss: 4.8480\n",
      "Epoch 23 Loss: 4.8066\n",
      "Epoch 24 Loss: 4.7707\n",
      "Epoch 25 Loss: 4.7185\n",
      "Epoch 26 Loss: 4.6901\n",
      "Epoch 27 Loss: 4.6478\n",
      "Epoch 28 Loss: 4.6146\n",
      "Epoch 29 Loss: 4.5757\n",
      "Epoch 30 Loss: 4.5393\n",
      "Epoch 31 Loss: 4.5127\n",
      "Epoch 32 Loss: 4.4848\n",
      "Epoch 33 Loss: 4.4409\n",
      "Epoch 34 Loss: 4.4135\n",
      "Epoch 35 Loss: 4.3868\n",
      "Epoch 36 Loss: 4.3427\n",
      "Epoch 37 Loss: 4.3185\n",
      "Epoch 38 Loss: 4.2958\n",
      "Epoch 39 Loss: 4.2631\n",
      "Epoch 40 Loss: 4.2321\n",
      "Epoch 41 Loss: 4.2038\n",
      "Epoch 42 Loss: 4.1820\n",
      "Epoch 43 Loss: 4.1555\n",
      "Epoch 44 Loss: 4.1351\n",
      "Epoch 45 Loss: 4.1129\n",
      "Epoch 46 Loss: 4.0887\n",
      "Epoch 47 Loss: 4.0638\n",
      "Epoch 48 Loss: 4.0361\n",
      "Epoch 49 Loss: 4.0149\n",
      "Epoch 50 Loss: 3.9972\n",
      "Epoch 51 Loss: 3.9676\n",
      "Epoch 52 Loss: 3.9548\n",
      "Epoch 53 Loss: 3.9216\n",
      "Epoch 54 Loss: 3.9068\n",
      "Epoch 55 Loss: 3.8810\n",
      "Epoch 56 Loss: 3.8616\n",
      "Epoch 57 Loss: 3.8442\n",
      "Epoch 58 Loss: 3.8205\n",
      "Epoch 59 Loss: 3.8097\n",
      "Epoch 60 Loss: 3.7845\n",
      "Epoch 61 Loss: 3.7668\n",
      "Epoch 62 Loss: 3.7471\n",
      "Epoch 63 Loss: 3.7283\n",
      "Epoch 64 Loss: 3.7164\n",
      "Epoch 65 Loss: 3.6844\n",
      "Epoch 66 Loss: 3.6724\n",
      "Epoch 67 Loss: 3.6521\n",
      "Epoch 68 Loss: 3.6383\n",
      "Epoch 69 Loss: 3.6135\n",
      "Epoch 70 Loss: 3.6020\n",
      "Epoch 71 Loss: 3.5878\n",
      "Epoch 72 Loss: 3.5690\n",
      "Epoch 73 Loss: 3.5505\n",
      "Epoch 74 Loss: 3.5330\n",
      "Epoch 75 Loss: 3.5220\n",
      "Epoch 76 Loss: 3.4979\n",
      "Epoch 77 Loss: 3.4891\n",
      "Epoch 78 Loss: 3.4623\n",
      "Epoch 79 Loss: 3.4526\n",
      "Epoch 80 Loss: 3.4360\n",
      "Epoch 81 Loss: 3.4184\n",
      "Epoch 82 Loss: 3.4002\n",
      "Epoch 83 Loss: 3.3861\n",
      "Epoch 84 Loss: 3.3675\n",
      "Epoch 85 Loss: 3.3605\n",
      "Epoch 86 Loss: 3.3377\n",
      "Epoch 87 Loss: 3.3266\n",
      "Epoch 88 Loss: 3.3077\n",
      "Epoch 89 Loss: 3.2924\n",
      "Epoch 90 Loss: 3.2819\n",
      "Epoch 91 Loss: 3.2695\n",
      "Epoch 92 Loss: 3.2514\n",
      "Epoch 93 Loss: 3.2324\n",
      "Epoch 94 Loss: 3.2183\n",
      "Epoch 95 Loss: 3.1991\n",
      "Epoch 96 Loss: 3.1928\n",
      "Epoch 97 Loss: 3.1764\n",
      "Epoch 98 Loss: 3.1505\n",
      "Epoch 99 Loss: 3.1459\n",
      "Epoch 100 Loss: 3.1420\n",
      "Epoch 101 Loss: 3.1126\n",
      "Epoch 102 Loss: 3.1083\n",
      "Epoch 103 Loss: 3.0896\n",
      "Epoch 104 Loss: 3.0741\n",
      "Epoch 105 Loss: 3.0632\n",
      "Epoch 106 Loss: 3.0413\n",
      "Epoch 107 Loss: 3.0382\n",
      "Epoch 108 Loss: 3.0258\n",
      "Epoch 109 Loss: 3.0006\n",
      "Epoch 110 Loss: 2.9855\n",
      "Epoch 111 Loss: 2.9752\n",
      "Epoch 112 Loss: 2.9663\n",
      "Epoch 113 Loss: 2.9503\n",
      "Epoch 114 Loss: 2.9407\n",
      "Epoch 115 Loss: 2.9223\n",
      "Epoch 116 Loss: 2.9148\n",
      "Epoch 117 Loss: 2.8946\n",
      "Epoch 118 Loss: 2.8888\n",
      "Epoch 119 Loss: 2.8671\n",
      "Epoch 120 Loss: 2.8562\n",
      "Epoch 121 Loss: 2.8414\n",
      "Epoch 122 Loss: 2.8240\n",
      "Epoch 123 Loss: 2.8099\n",
      "Epoch 124 Loss: 2.8070\n",
      "Epoch 125 Loss: 2.7836\n",
      "Epoch 126 Loss: 2.7728\n",
      "Epoch 127 Loss: 2.7645\n",
      "Epoch 128 Loss: 2.7415\n",
      "Epoch 129 Loss: 2.7424\n",
      "Epoch 130 Loss: 2.7281\n",
      "Epoch 131 Loss: 2.7098\n",
      "Epoch 132 Loss: 2.6943\n",
      "Epoch 133 Loss: 2.6897\n",
      "Epoch 134 Loss: 2.6722\n",
      "Epoch 135 Loss: 2.6614\n",
      "Epoch 136 Loss: 2.6501\n",
      "Epoch 137 Loss: 2.6421\n",
      "Epoch 138 Loss: 2.6213\n",
      "Epoch 139 Loss: 2.6190\n",
      "Epoch 140 Loss: 2.5997\n",
      "Epoch 141 Loss: 2.5864\n",
      "Epoch 142 Loss: 2.5798\n",
      "Epoch 143 Loss: 2.5701\n",
      "Epoch 144 Loss: 2.5466\n",
      "Epoch 145 Loss: 2.5352\n",
      "Epoch 146 Loss: 2.5271\n",
      "Epoch 147 Loss: 2.5172\n",
      "Epoch 148 Loss: 2.5039\n",
      "Epoch 149 Loss: 2.4923\n",
      "Epoch 150 Loss: 2.4808\n",
      "Epoch 151 Loss: 2.4615\n",
      "Epoch 152 Loss: 2.4634\n",
      "Epoch 153 Loss: 2.4385\n",
      "Epoch 154 Loss: 2.4353\n",
      "Epoch 155 Loss: 2.4251\n",
      "Epoch 156 Loss: 2.4073\n",
      "Epoch 157 Loss: 2.4013\n",
      "Epoch 158 Loss: 2.3835\n",
      "Epoch 159 Loss: 2.3644\n",
      "Epoch 160 Loss: 2.3636\n",
      "Epoch 161 Loss: 2.3486\n",
      "Epoch 162 Loss: 2.3422\n",
      "Epoch 163 Loss: 2.3290\n",
      "Epoch 164 Loss: 2.3138\n",
      "Epoch 165 Loss: 2.3084\n",
      "Epoch 166 Loss: 2.2933\n",
      "Epoch 167 Loss: 2.2946\n",
      "Epoch 168 Loss: 2.2670\n",
      "Epoch 169 Loss: 2.2561\n",
      "Epoch 170 Loss: 2.2409\n",
      "Epoch 171 Loss: 2.2362\n",
      "Epoch 172 Loss: 2.2287\n",
      "Epoch 173 Loss: 2.2123\n",
      "Epoch 174 Loss: 2.2029\n",
      "Epoch 175 Loss: 2.1939\n",
      "Epoch 176 Loss: 2.1855\n",
      "Epoch 177 Loss: 2.1663\n",
      "Epoch 178 Loss: 2.1597\n",
      "Epoch 179 Loss: 2.1499\n",
      "Epoch 180 Loss: 2.1303\n",
      "Epoch 181 Loss: 2.1166\n",
      "Epoch 182 Loss: 2.1033\n",
      "Epoch 183 Loss: 2.1057\n",
      "Epoch 184 Loss: 2.0906\n",
      "Epoch 185 Loss: 2.0784\n",
      "Epoch 186 Loss: 2.0648\n",
      "Epoch 187 Loss: 2.0605\n",
      "Epoch 188 Loss: 2.0617\n",
      "Epoch 189 Loss: 2.0422\n",
      "Epoch 190 Loss: 2.0365\n",
      "Epoch 191 Loss: 2.0137\n",
      "Epoch 192 Loss: 2.0180\n",
      "Epoch 193 Loss: 1.9993\n",
      "Epoch 194 Loss: 1.9908\n",
      "Epoch 195 Loss: 1.9775\n",
      "Epoch 196 Loss: 1.9664\n",
      "Epoch 197 Loss: 1.9471\n",
      "Epoch 198 Loss: 1.9531\n",
      "Epoch 199 Loss: 1.9381\n",
      "Epoch 200 Loss: 1.9264\n",
      "Epoch 201 Loss: 1.9103\n",
      "Epoch 202 Loss: 1.9133\n",
      "Epoch 203 Loss: 1.9021\n",
      "Epoch 204 Loss: 1.8775\n",
      "Epoch 205 Loss: 1.8760\n",
      "Epoch 206 Loss: 1.8700\n",
      "Epoch 207 Loss: 1.8558\n",
      "Epoch 208 Loss: 1.8435\n",
      "Epoch 209 Loss: 1.8426\n",
      "Epoch 210 Loss: 1.8288\n",
      "Epoch 211 Loss: 1.8077\n",
      "Epoch 212 Loss: 1.8037\n",
      "Epoch 213 Loss: 1.8090\n",
      "Epoch 214 Loss: 1.7830\n",
      "Epoch 215 Loss: 1.7704\n",
      "Epoch 216 Loss: 1.7683\n",
      "Epoch 217 Loss: 1.7539\n",
      "Epoch 218 Loss: 1.7458\n",
      "Epoch 219 Loss: 1.7439\n",
      "Epoch 220 Loss: 1.7288\n",
      "Epoch 221 Loss: 1.7215\n",
      "Epoch 222 Loss: 1.7111\n",
      "Epoch 223 Loss: 1.6976\n",
      "Epoch 224 Loss: 1.6777\n",
      "Epoch 225 Loss: 1.6729\n",
      "Epoch 226 Loss: 1.6702\n",
      "Epoch 227 Loss: 1.6576\n",
      "Epoch 228 Loss: 1.6598\n",
      "Epoch 229 Loss: 1.6418\n",
      "Epoch 230 Loss: 1.6341\n",
      "Epoch 231 Loss: 1.6333\n",
      "Epoch 232 Loss: 1.6251\n",
      "Epoch 233 Loss: 1.6046\n",
      "Epoch 234 Loss: 1.5946\n",
      "Epoch 235 Loss: 1.5936\n",
      "Epoch 236 Loss: 1.5890\n",
      "Epoch 237 Loss: 1.5773\n",
      "Epoch 238 Loss: 1.5639\n",
      "Epoch 239 Loss: 1.5594\n",
      "Epoch 240 Loss: 1.5432\n",
      "Epoch 241 Loss: 1.5354\n",
      "Epoch 242 Loss: 1.5374\n",
      "Epoch 243 Loss: 1.5232\n",
      "Epoch 244 Loss: 1.5105\n",
      "Epoch 245 Loss: 1.5103\n",
      "Epoch 246 Loss: 1.4964\n",
      "Epoch 247 Loss: 1.4901\n",
      "Epoch 248 Loss: 1.4873\n",
      "Epoch 249 Loss: 1.4672\n",
      "Epoch 250 Loss: 1.4650\n",
      "Epoch 251 Loss: 1.4540\n",
      "Epoch 252 Loss: 1.4449\n",
      "Epoch 253 Loss: 1.4359\n",
      "Epoch 254 Loss: 1.4299\n",
      "Epoch 255 Loss: 1.4184\n",
      "Epoch 256 Loss: 1.4156\n",
      "Epoch 257 Loss: 1.4033\n",
      "Epoch 258 Loss: 1.3998\n",
      "Epoch 259 Loss: 1.3948\n",
      "Epoch 260 Loss: 1.3933\n",
      "Epoch 261 Loss: 1.3785\n",
      "Epoch 262 Loss: 1.3610\n",
      "Epoch 263 Loss: 1.3608\n",
      "Epoch 264 Loss: 1.3555\n",
      "Epoch 265 Loss: 1.3492\n",
      "Epoch 266 Loss: 1.3311\n",
      "Epoch 267 Loss: 1.3185\n",
      "Epoch 268 Loss: 1.3297\n",
      "Epoch 269 Loss: 1.3179\n",
      "Epoch 270 Loss: 1.3039\n",
      "Epoch 271 Loss: 1.2903\n",
      "Epoch 272 Loss: 1.2830\n",
      "Epoch 273 Loss: 1.2730\n",
      "Epoch 274 Loss: 1.2803\n",
      "Epoch 275 Loss: 1.2689\n",
      "Epoch 276 Loss: 1.2679\n",
      "Epoch 277 Loss: 1.2485\n",
      "Epoch 278 Loss: 1.2446\n",
      "Epoch 279 Loss: 1.2388\n",
      "Epoch 280 Loss: 1.2321\n",
      "Epoch 281 Loss: 1.2266\n",
      "Epoch 282 Loss: 1.2150\n",
      "Epoch 283 Loss: 1.2113\n",
      "Epoch 284 Loss: 1.2037\n",
      "Epoch 285 Loss: 1.1969\n",
      "Epoch 286 Loss: 1.1863\n",
      "Epoch 287 Loss: 1.1868\n",
      "Epoch 288 Loss: 1.1789\n",
      "Epoch 289 Loss: 1.1688\n",
      "Epoch 290 Loss: 1.1585\n",
      "Epoch 291 Loss: 1.1554\n",
      "Epoch 292 Loss: 1.1491\n",
      "Epoch 293 Loss: 1.1446\n",
      "Epoch 294 Loss: 1.1269\n",
      "Epoch 295 Loss: 1.1292\n",
      "Epoch 296 Loss: 1.1256\n",
      "Epoch 297 Loss: 1.1164\n",
      "Epoch 298 Loss: 1.1060\n",
      "Epoch 299 Loss: 1.0963\n",
      "Epoch 300 Loss: 1.0988\n",
      "Epoch 301 Loss: 1.0866\n",
      "Epoch 302 Loss: 1.0750\n",
      "Epoch 303 Loss: 1.0708\n",
      "Epoch 304 Loss: 1.0708\n",
      "Epoch 305 Loss: 1.0577\n",
      "Epoch 306 Loss: 1.0581\n",
      "Epoch 307 Loss: 1.0459\n",
      "Epoch 308 Loss: 1.0427\n",
      "Epoch 309 Loss: 1.0321\n",
      "Epoch 310 Loss: 1.0187\n",
      "Epoch 311 Loss: 1.0303\n",
      "Epoch 312 Loss: 1.0113\n",
      "Epoch 313 Loss: 1.0164\n",
      "Epoch 314 Loss: 1.0129\n",
      "Epoch 315 Loss: 1.0032\n",
      "Epoch 316 Loss: 0.9899\n",
      "Epoch 317 Loss: 0.9957\n",
      "Epoch 318 Loss: 0.9809\n",
      "Epoch 319 Loss: 0.9748\n",
      "Epoch 320 Loss: 0.9748\n",
      "Epoch 321 Loss: 0.9618\n",
      "Epoch 322 Loss: 0.9551\n",
      "Epoch 323 Loss: 0.9643\n",
      "Epoch 324 Loss: 0.9516\n",
      "Epoch 325 Loss: 0.9370\n",
      "Epoch 326 Loss: 0.9344\n",
      "Epoch 327 Loss: 0.9305\n",
      "Epoch 328 Loss: 0.9175\n",
      "Epoch 329 Loss: 0.9173\n",
      "Epoch 330 Loss: 0.9143\n",
      "Epoch 331 Loss: 0.9126\n",
      "Epoch 332 Loss: 0.8976\n",
      "Epoch 333 Loss: 0.8989\n",
      "Epoch 334 Loss: 0.8852\n",
      "Epoch 335 Loss: 0.8899\n",
      "Epoch 336 Loss: 0.8765\n",
      "Epoch 337 Loss: 0.8644\n",
      "Epoch 338 Loss: 0.8731\n",
      "Epoch 339 Loss: 0.8644\n",
      "Epoch 340 Loss: 0.8541\n",
      "Epoch 341 Loss: 0.8506\n",
      "Epoch 342 Loss: 0.8509\n",
      "Epoch 343 Loss: 0.8459\n",
      "Epoch 344 Loss: 0.8390\n",
      "Epoch 345 Loss: 0.8398\n",
      "Epoch 346 Loss: 0.8210\n",
      "Epoch 347 Loss: 0.8241\n",
      "Epoch 348 Loss: 0.8244\n",
      "Epoch 349 Loss: 0.8068\n",
      "Epoch 350 Loss: 0.8109\n",
      "Epoch 351 Loss: 0.7948\n",
      "Epoch 352 Loss: 0.8126\n",
      "Epoch 353 Loss: 0.7941\n",
      "Epoch 354 Loss: 0.7820\n",
      "Epoch 355 Loss: 0.7774\n",
      "Epoch 356 Loss: 0.7803\n",
      "Epoch 357 Loss: 0.7703\n",
      "Epoch 358 Loss: 0.7672\n",
      "Epoch 359 Loss: 0.7653\n",
      "Epoch 360 Loss: 0.7644\n",
      "Epoch 361 Loss: 0.7635\n",
      "Epoch 362 Loss: 0.7616\n",
      "Epoch 363 Loss: 0.7510\n",
      "Epoch 364 Loss: 0.7480\n",
      "Epoch 365 Loss: 0.7373\n",
      "Epoch 366 Loss: 0.7318\n",
      "Epoch 367 Loss: 0.7261\n",
      "Epoch 368 Loss: 0.7197\n",
      "Epoch 369 Loss: 0.7139\n",
      "Epoch 370 Loss: 0.7147\n",
      "Epoch 371 Loss: 0.7151\n",
      "Epoch 372 Loss: 0.7058\n",
      "Epoch 373 Loss: 0.7078\n",
      "Epoch 374 Loss: 0.6933\n",
      "Epoch 375 Loss: 0.6916\n",
      "Epoch 376 Loss: 0.7030\n",
      "Epoch 377 Loss: 0.6900\n",
      "Epoch 378 Loss: 0.6791\n",
      "Epoch 379 Loss: 0.6785\n",
      "Epoch 380 Loss: 0.6733\n",
      "Epoch 381 Loss: 0.6751\n",
      "Epoch 382 Loss: 0.6662\n",
      "Epoch 383 Loss: 0.6594\n",
      "Epoch 384 Loss: 0.6584\n",
      "Epoch 385 Loss: 0.6492\n",
      "Epoch 386 Loss: 0.6410\n",
      "Epoch 387 Loss: 0.6440\n",
      "Epoch 388 Loss: 0.6446\n",
      "Epoch 389 Loss: 0.6337\n",
      "Epoch 390 Loss: 0.6333\n",
      "Epoch 391 Loss: 0.6331\n",
      "Epoch 392 Loss: 0.6275\n",
      "Epoch 393 Loss: 0.6297\n",
      "Epoch 394 Loss: 0.6194\n",
      "Epoch 395 Loss: 0.6189\n",
      "Epoch 396 Loss: 0.6034\n",
      "Epoch 397 Loss: 0.6079\n",
      "Epoch 398 Loss: 0.6053\n",
      "Epoch 399 Loss: 0.5988\n",
      "Epoch 400 Loss: 0.5986\n",
      "Epoch 401 Loss: 0.5903\n",
      "Epoch 402 Loss: 0.5939\n",
      "Epoch 403 Loss: 0.5776\n",
      "Epoch 404 Loss: 0.5816\n",
      "Epoch 405 Loss: 0.5843\n",
      "Epoch 406 Loss: 0.5754\n",
      "Epoch 407 Loss: 0.5803\n",
      "Epoch 408 Loss: 0.5687\n",
      "Epoch 409 Loss: 0.5693\n",
      "Epoch 410 Loss: 0.5592\n",
      "Epoch 411 Loss: 0.5557\n",
      "Epoch 412 Loss: 0.5574\n",
      "Epoch 413 Loss: 0.5609\n",
      "Epoch 414 Loss: 0.5474\n",
      "Epoch 415 Loss: 0.5347\n",
      "Epoch 416 Loss: 0.5402\n",
      "Epoch 417 Loss: 0.5432\n",
      "Epoch 418 Loss: 0.5323\n",
      "Epoch 419 Loss: 0.5361\n",
      "Epoch 420 Loss: 0.5265\n",
      "Epoch 421 Loss: 0.5237\n",
      "Epoch 422 Loss: 0.5238\n",
      "Epoch 423 Loss: 0.5245\n",
      "Epoch 424 Loss: 0.5328\n",
      "Epoch 425 Loss: 0.5169\n",
      "Epoch 426 Loss: 0.5137\n",
      "Epoch 427 Loss: 0.5135\n",
      "Epoch 428 Loss: 0.5107\n",
      "Epoch 429 Loss: 0.5149\n",
      "Epoch 430 Loss: 0.4966\n",
      "Epoch 431 Loss: 0.4929\n",
      "Epoch 432 Loss: 0.4965\n",
      "Epoch 433 Loss: 0.4901\n",
      "Epoch 434 Loss: 0.4918\n",
      "Epoch 435 Loss: 0.4922\n",
      "Epoch 436 Loss: 0.4831\n",
      "Epoch 437 Loss: 0.4863\n",
      "Epoch 438 Loss: 0.4800\n",
      "Epoch 439 Loss: 0.4880\n",
      "Epoch 440 Loss: 0.4783\n",
      "Epoch 441 Loss: 0.4656\n",
      "Epoch 442 Loss: 0.4705\n",
      "Epoch 443 Loss: 0.4729\n",
      "Epoch 444 Loss: 0.4615\n",
      "Epoch 445 Loss: 0.4659\n",
      "Epoch 446 Loss: 0.4574\n",
      "Epoch 447 Loss: 0.4636\n",
      "Epoch 448 Loss: 0.4518\n",
      "Epoch 449 Loss: 0.4582\n",
      "Epoch 450 Loss: 0.4530\n",
      "Epoch 451 Loss: 0.4469\n",
      "Epoch 452 Loss: 0.4423\n",
      "Epoch 453 Loss: 0.4396\n",
      "Epoch 454 Loss: 0.4391\n",
      "Epoch 455 Loss: 0.4456\n",
      "Epoch 456 Loss: 0.4284\n",
      "Epoch 457 Loss: 0.4309\n",
      "Epoch 458 Loss: 0.4305\n",
      "Epoch 459 Loss: 0.4292\n",
      "Epoch 460 Loss: 0.4303\n",
      "Epoch 461 Loss: 0.4190\n",
      "Epoch 462 Loss: 0.4330\n",
      "Epoch 463 Loss: 0.4129\n",
      "Epoch 464 Loss: 0.4063\n",
      "Epoch 465 Loss: 0.4138\n",
      "Epoch 466 Loss: 0.4216\n",
      "Epoch 467 Loss: 0.4106\n",
      "Epoch 468 Loss: 0.3991\n",
      "Epoch 469 Loss: 0.4018\n",
      "Epoch 470 Loss: 0.3986\n",
      "Epoch 471 Loss: 0.3993\n",
      "Epoch 472 Loss: 0.3931\n",
      "Epoch 473 Loss: 0.3972\n",
      "Epoch 474 Loss: 0.3924\n",
      "Epoch 475 Loss: 0.3976\n",
      "Epoch 476 Loss: 0.3978\n",
      "Epoch 477 Loss: 0.3917\n",
      "Epoch 478 Loss: 0.3808\n",
      "Epoch 479 Loss: 0.3840\n",
      "Epoch 480 Loss: 0.3756\n",
      "Epoch 481 Loss: 0.3854\n",
      "Epoch 482 Loss: 0.3722\n",
      "Epoch 483 Loss: 0.3689\n",
      "Epoch 484 Loss: 0.3679\n",
      "Epoch 485 Loss: 0.3651\n",
      "Epoch 486 Loss: 0.3710\n",
      "Epoch 487 Loss: 0.3690\n",
      "Epoch 488 Loss: 0.3595\n",
      "Epoch 489 Loss: 0.3554\n",
      "Epoch 490 Loss: 0.3606\n",
      "Epoch 491 Loss: 0.3597\n",
      "Epoch 492 Loss: 0.3605\n",
      "Epoch 493 Loss: 0.3624\n",
      "Epoch 494 Loss: 0.3540\n",
      "Epoch 495 Loss: 0.3558\n",
      "Epoch 496 Loss: 0.3557\n",
      "Epoch 497 Loss: 0.3451\n",
      "Epoch 498 Loss: 0.3392\n",
      "Epoch 499 Loss: 0.3492\n",
      "Epoch 500 Loss: 0.3437\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load SpaCy tokenizers\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "# Load Multi30k train data\n",
    "def load_multi30k_from_folder(path):\n",
    "    with open(f\"{path}/en.txt\", encoding=\"utf-8\") as f_en, open(f\"{path}/de.txt\", encoding=\"utf-8\") as f_de:\n",
    "        en_sentences = f_en.read().strip().split('\\n')\n",
    "        de_sentences = f_de.read().strip().split('\\n')\n",
    "    return list(zip(en_sentences, de_sentences))\n",
    "\n",
    "def load_tatoeba_parallel(path):\n",
    "    # Assuming tab-separated file (or whitespace-separated)\n",
    "    df = pd.read_csv(path, sep='\\t', header=None, usecols=[0,1], names=['en', 'de'], encoding='utf-8', nrows = 50000)\n",
    "    \n",
    "    # Convert to list of tuples (en, de)\n",
    "    pairs = list(zip(df['en'].tolist(), df['de'].tolist()))\n",
    "    return pairs\n",
    "\n",
    "data = load_tatoeba_parallel(\"archive/deu.txt\")\n",
    "\n",
    "# data = load_multi30k_from_folder(\"data\")  # Change path as needed\n",
    "print(f\"Loaded {len(data)} sentence pairs\")\n",
    "\n",
    "# Special tokens\n",
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "# Build vocabularies manually\n",
    "def build_vocab_manual(data, index, tokenizer, specials=SPECIAL_TOKENS, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for pair in data:\n",
    "        tokens = tokenizer(pair[index])\n",
    "        counter.update(tokens)\n",
    "    itos = list(specials)\n",
    "    itos += [tok for tok, freq in counter.items() if freq >= min_freq and tok not in specials]\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "src_stoi, src_itos = build_vocab_manual(data, 0, tokenize_en)\n",
    "tgt_stoi, tgt_itos = build_vocab_manual(data, 1, tokenize_de)\n",
    "\n",
    "PAD_IDX = src_stoi[\"<pad>\"]\n",
    "UNK_IDX = src_stoi[\"<unk>\"]\n",
    "SOS_IDX = src_stoi[\"<sos>\"]\n",
    "EOS_IDX = src_stoi[\"<eos>\"]\n",
    "\n",
    "# Dataset class using manual stoi dicts\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, src_stoi, tgt_stoi, src_tokenizer, tgt_tokenizer):\n",
    "        self.data = data\n",
    "        self.src_stoi = src_stoi\n",
    "        self.tgt_stoi = tgt_stoi\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence, tgt_sentence = self.data[idx]\n",
    "        src_tokens = [\"<sos>\"] + self.src_tokenizer(src_sentence) + [\"<eos>\"]\n",
    "        tgt_tokens = [\"<sos>\"] + self.tgt_tokenizer(tgt_sentence) + [\"<eos>\"]\n",
    "\n",
    "        src_ids = torch.tensor([self.src_stoi.get(tok, UNK_IDX) for tok in src_tokens], dtype=torch.long)\n",
    "        tgt_ids = torch.tensor([self.tgt_stoi.get(tok, UNK_IDX) for tok in tgt_tokens], dtype=torch.long)\n",
    "\n",
    "        return src_ids, tgt_ids\n",
    "\n",
    "# Collate function for padding batches\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# Prepare Dataset and DataLoader\n",
    "train_dataset = TranslationDataset(data, src_stoi, tgt_stoi, tokenize_en, tokenize_de)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Example model setup (replace with your actual Transformer)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(src_stoi)\n",
    "TGT_VOCAB_SIZE = len(tgt_stoi)\n",
    "MAX_LEN = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = Transformer(\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    d_model=128,\n",
    "    num_heads=2,\n",
    "    num_layers=1,\n",
    "    d_ff=256,\n",
    "    max_seq_length=MAX_LEN,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Use the manual vocab dict for padding index here:\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_stoi[\"<pad>\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        preds = model(src, tgt_input)\n",
    "        preds = preds.reshape(-1, preds.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        loss = criterion(preds, tgt_output)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6303041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: <sos> ein basketballspieler wirft auf den korb . <eos>\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, src_sentence, src_stoi, tgt_stoi, tgt_itos, max_len=50, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and convert source sentence using your tokenizer, here assumed pre-tokenized list of ints\n",
    "    src_tensor = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(device)  # (1, src_len)\n",
    "    \n",
    "    sos_token_id = tgt_stoi[\"<sos>\"]\n",
    "    eos_token_id = tgt_stoi[\"<eos>\"]\n",
    "    \n",
    "    tgt_tensor = torch.tensor([[sos_token_id]], dtype=torch.long).to(device)  # (1,1)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, tgt_tensor)  # (1, tgt_len, vocab_size)\n",
    "        \n",
    "        next_token_logits = output[0, -1, :]  # logits for last token\n",
    "        next_token_id = torch.argmax(next_token_logits).item()\n",
    "        \n",
    "        tgt_tensor = torch.cat([tgt_tensor, torch.tensor([[next_token_id]], device=device)], dim=1)\n",
    "        \n",
    "        if next_token_id == eos_token_id:\n",
    "            break\n",
    "    \n",
    "    predicted_ids = tgt_tensor[0].tolist()\n",
    "    predicted_tokens = [tgt_itos[i] if i < len(tgt_itos) else \"<unk>\" for i in predicted_ids]\n",
    "    \n",
    "    return predicted_tokens\n",
    "\n",
    "# Use your actual tokenizer here instead of split()\n",
    "src_text = \"A basketball player is taking a shot.\"\n",
    "src_tokens = tokenize_en(src_text)  # <-- your actual tokenizer function\n",
    "src_ids = [src_stoi.get(tok, src_stoi[\"<unk>\"]) for tok in src_tokens]\n",
    "\n",
    "result = evaluate(model, src_ids, src_stoi, tgt_stoi, tgt_itos, device=device)\n",
    "print(\"Generated:\", \" \".join(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c724d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
