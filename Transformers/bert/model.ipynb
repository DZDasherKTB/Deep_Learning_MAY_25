{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104dbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a22499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model,num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    assert d_model % num_heads == 0\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_heads = num_heads\n",
    "    self.d_k = d_model // num_heads\n",
    "\n",
    "    self.W_q = nn.Linear(d_model, d_model)\n",
    "    self.W_k = nn.Linear(d_model, d_model)\n",
    "    self.W_v = nn.Linear(d_model, d_model)\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def scaled_dot_product_attention(self, Q, K, V, mask = None):\n",
    "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "      attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    attn_probs = torch.softmax(attn_scores, dim = -1)\n",
    "\n",
    "    output = torch.matmul(attn_probs, V)\n",
    "    return output\n",
    "  \n",
    "  def split_heads(self, x):\n",
    "    batch_size, seq_length, embed_dim = x.size()\n",
    "    x = x.view(batch_size, seq_length, self.num_heads, self.d_k)\n",
    "    return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "  def combine_heads(self, x):\n",
    "    batch_size, _, seq_length, d_k = x.size()\n",
    "    return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "  def forward(self, Q,K,V, mask = None):\n",
    "    Q = self.split_heads(self.W_q(Q))\n",
    "    K = self.split_heads(self.W_q(K))\n",
    "    V = self.split_heads(self.W_q(V))\n",
    "\n",
    "\n",
    "    attn_output = self.scaled_dot_product_attention(Q,K,V,mask)\n",
    "    \n",
    "    output = self.W_o(self.combine_heads(attn_output))\n",
    "    return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "  def __init__(self, d_model, d_ff):\n",
    "    super(PositionWiseFeedForward,self).__init__()\n",
    "    self.fc1 = nn.Linear(d_model, d_ff)\n",
    "    self.fc2 = nn.Linear(d_ff,d_model)\n",
    "    self.relu = nn.ReLU()\n",
    "  def forward(self, x):\n",
    "    return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self,d_model,max_seq_length):\n",
    "    super(PositionalEncoding,self).__init__()\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "    position = torch.arange(0,max_seq_length,dtype = torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0,d_model,2).float()* -(math.log(10000.0)/d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_model,d_ff)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  def forward(self, x, mask):\n",
    "    attn_output = self.self_attn(x, x, x, mask)\n",
    "    x = self.norm1(x +self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm2(x +self.dropout(ff_output))\n",
    "    return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self,d_model, num_heads, d_ff, dropout):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.self_attn = MultiHeadAttention(d_model,num_heads)\n",
    "    self.cross_attn = MultiHeadAttention(d_model,num_heads)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.norm3 = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "    attn_output = self.self_attn(x,x,x, tgt_mask)\n",
    "    x = self.norm1(x + self.dropout(attn_output))\n",
    "    attn_output = self.cross_attn(x,enc_output,enc_output,src_mask)\n",
    "    x = self.norm2(x + self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm3(x + self.dropout(ff_output))\n",
    "    return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads,num_layers,d_ff,max_seq_length,dropout):\n",
    "    super(Transformer,self).__init__()\n",
    "    self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "    self.decoder_embedding = nn.Embedding(tgt_vocab_size,d_model)\n",
    "    self.positional_encoding = PositionalEncoding(d_model,max_seq_length)\n",
    "    self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "    self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "    self.fc = nn.Linear(d_model,tgt_vocab_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def generate_mask(self, src, tgt):\n",
    "    device = src.device \n",
    "\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(device)  # ensure device\n",
    "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(device)\n",
    "\n",
    "    seq_length = tgt.size(1)\n",
    "\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
    "\n",
    "    tgt_mask = tgt_mask & nopeak_mask\n",
    "\n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "  def forward(self,src,tgt):\n",
    "    src_mask,tgt_mask = self.generate_mask(src,tgt)\n",
    "    src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "    tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "    enc_output = src_embedded\n",
    "    for enc_layer in self.encoder_layers:\n",
    "      enc_output = enc_layer(enc_output,src_mask)\n",
    "    \n",
    "    dec_output = tgt_embedded\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "    output = self.fc(dec_output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222121cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4171c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 221533 sentence pairs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Load SpaCy tokenizers\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "# Load Multi30k train data\n",
    "def load_multi30k_from_folder(path):\n",
    "    with open(f\"{path}/en.txt\", encoding=\"utf-8\") as f_en, open(f\"{path}/de.txt\", encoding=\"utf-8\") as f_de:\n",
    "        en_sentences = f_en.read().strip().split('\\n')\n",
    "        de_sentences = f_de.read().strip().split('\\n')\n",
    "    return list(zip(en_sentences, de_sentences))\n",
    "\n",
    "def load_tatoeba_parallel(path):\n",
    "    # Assuming tab-separated file (or whitespace-separated)\n",
    "    df = pd.read_csv(path, sep='\\t', header=None, usecols=[0,1], names=['en', 'de'], encoding='utf-8')\n",
    "    \n",
    "    # Convert to list of tuples (en, de)\n",
    "    pairs = list(zip(df['en'].tolist(), df['de'].tolist()))\n",
    "    return pairs\n",
    "\n",
    "data = load_tatoeba_parallel(\"archive/deu.txt\")\n",
    "\n",
    "# data = load_multi30k_from_folder(\"data\")  # Change path as needed\n",
    "print(f\"Loaded {len(data)} sentence pairs\")\n",
    "\n",
    "# Special tokens\n",
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "# Build vocabularies manually\n",
    "def build_vocab_manual(data, index, tokenizer, specials=SPECIAL_TOKENS, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for pair in data:\n",
    "        tokens = tokenizer(pair[index])\n",
    "        counter.update(tokens)\n",
    "    itos = list(specials)\n",
    "    itos += [tok for tok, freq in counter.items() if freq >= min_freq and tok not in specials]\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "src_stoi, src_itos = build_vocab_manual(data, 0, tokenize_en)\n",
    "tgt_stoi, tgt_itos = build_vocab_manual(data, 1, tokenize_de)\n",
    "\n",
    "PAD_IDX = src_stoi[\"<pad>\"]\n",
    "UNK_IDX = src_stoi[\"<unk>\"]\n",
    "SOS_IDX = src_stoi[\"<sos>\"]\n",
    "EOS_IDX = src_stoi[\"<eos>\"]\n",
    "\n",
    "# Dataset class using manual stoi dicts\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, src_stoi, tgt_stoi, src_tokenizer, tgt_tokenizer):\n",
    "        self.data = data\n",
    "        self.src_stoi = src_stoi\n",
    "        self.tgt_stoi = tgt_stoi\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence, tgt_sentence = self.data[idx]\n",
    "        src_tokens = [\"<sos>\"] + self.src_tokenizer(src_sentence) + [\"<eos>\"]\n",
    "        tgt_tokens = [\"<sos>\"] + self.tgt_tokenizer(tgt_sentence) + [\"<eos>\"]\n",
    "\n",
    "        src_ids = torch.tensor([self.src_stoi.get(tok, UNK_IDX) for tok in src_tokens], dtype=torch.long)\n",
    "        tgt_ids = torch.tensor([self.tgt_stoi.get(tok, UNK_IDX) for tok in tgt_tokens], dtype=torch.long)\n",
    "\n",
    "        return src_ids, tgt_ids\n",
    "\n",
    "# Collate function for padding batches\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "SRC_VOCAB_SIZE = len(src_stoi)\n",
    "TGT_VOCAB_SIZE = len(tgt_stoi)\n",
    "MAX_LEN = 100\n",
    "dataset = TranslationDataset(data, src_stoi, tgt_stoi, tokenize_en, tokenize_de)\n",
    "\n",
    "# Split dataset: 80% train, 20% val (adjust as needed)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a90bed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Allocated: 1.96 GB\n",
      "Reserved : 3.17 GB\n",
      "Max Allocated: 5.00 GB\n",
      "Max Reserved : 11.92 GB\n",
      "Epoch 1 Train Loss: 4.0575 | Val Loss: 3.1450\n",
      "Epoch 2 Train Loss: 3.1308 | Val Loss: 2.6562\n",
      "Epoch 3 Train Loss: 2.7901 | Val Loss: 2.4084\n",
      "Epoch 4 Train Loss: 2.5910 | Val Loss: 2.3680\n",
      "Epoch 5 Train Loss: 2.4510 | Val Loss: 2.1988\n",
      "Epoch 6 Train Loss: 2.3472 | Val Loss: 2.1360\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m teacher_forcing_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# e.g. 70% teacher forcing\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# ground-truth input tokens\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 90\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     88\u001b[0m src_batch, tgt_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     89\u001b[0m src_batch \u001b[38;5;241m=\u001b[39m pad_sequence(src_batch, padding_value\u001b[38;5;241m=\u001b[39mPAD_IDX, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 90\u001b[0m tgt_batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPAD_IDX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m src_batch, tgt_batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\utils\\rnn.py:475\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value, padding_side)\u001b[0m\n\u001b[0;32m    471\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Model, optimizer, criterion remain the same\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = Transformer(\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    d_model=512,\n",
    "    num_heads=4,\n",
    "    num_layers=3,\n",
    "    d_ff=1024,\n",
    "    max_seq_length=150,\n",
    "    dropout=0.2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_stoi[\"<pad>\"])\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved : {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "print(f\"Max Allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Max Reserved : {torch.cuda.max_memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "for epoch in range(20):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    teacher_forcing_ratio = 0.5  # e.g. 70% teacher forcing\n",
    "\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]   # ground-truth input tokens\n",
    "        tgt_output = tgt[:, 1:]   # expected output tokens\n",
    "        \n",
    "        batch_size, seq_len = tgt_input.shape\n",
    "        \n",
    "        # Forward pass once with ground-truth for initial prediction\n",
    "        preds = model(src, tgt_input)  # [batch, seq_len, vocab_size]\n",
    "        preds_tokens = preds.argmax(dim=-1)  # greedy predictions: [batch, seq_len]\n",
    "        \n",
    "        # Decide for each token whether to use ground truth or predicted token\n",
    "        mask = torch.rand(batch_size, seq_len, device=device) < teacher_forcing_ratio\n",
    "        \n",
    "        # Construct mixed tgt_input:\n",
    "        mixed_tgt_input = torch.where(mask, tgt_input, preds_tokens)\n",
    "        \n",
    "        # Forward pass with mixed input\n",
    "        preds = model(src, mixed_tgt_input)\n",
    "        preds = preds.reshape(-1, preds.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        \n",
    "        loss = criterion(preds, tgt_output)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            preds = model(src, tgt_input)\n",
    "            preds = preds.reshape(-1, preds.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "            val_loss = criterion(preds, tgt_output)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "55ce4987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to model_epoch_7.pth\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Saved checkpoint to {path}\")\n",
    "save_checkpoint(model, optimizer, epoch, path=\"model_epoch_{}.pth\".format(epoch+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2529ab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Saved tokenizer and datasets to save_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "save_dict = {\n",
    "    \"src_stoi\": src_stoi,\n",
    "    \"tgt_stoi\": tgt_stoi,\n",
    "    \"src_itos\": src_itos if 'src_itos' in locals() else None,\n",
    "    \"tgt_itos\": tgt_itos if 'tgt_itos' in locals() else None,\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"val_dataset\": val_dataset\n",
    "}\n",
    "\n",
    "with open(\"save_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(save_dict, f)\n",
    "\n",
    "print(\"✔️ Saved tokenizer and datasets to save_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63404156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from model_epoch_19+4.pth, starting at epoch 4\n"
     ]
    }
   ],
   "source": [
    "def load_checkpoint(model, optimizer, path=\"checkpoint.pth\", device='cuda'):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint.get(\"epoch\", 0)\n",
    "    print(f\"Loaded checkpoint from {path}, starting at epoch {epoch+1}\")\n",
    "    return epoch\n",
    "start_epoch = load_checkpoint(model, optimizer, path=\"model_epoch_19+4.pth\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6303041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: <sos> du kennst sie ? ? <eos>\n"
     ]
    }
   ],
   "source": [
    "def evaluate_with_beam_search(model, src_sentence, src_stoi, tgt_stoi, tgt_itos,\n",
    "                              beam_width=8, max_len=50, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    sos_token_id = tgt_stoi[\"<sos>\"]\n",
    "    eos_token_id = tgt_stoi[\"<eos>\"]\n",
    "\n",
    "    src_tensor = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Initialize beam with sos token\n",
    "    beams = [(torch.tensor([[sos_token_id]], device=device), 0.0)]  # (sequence, score)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "                if seq[0, -1].item() == eos_token_id:\n",
    "                    # Already ended beam — keep it as-is\n",
    "                    new_beams.append((seq, score))\n",
    "                    continue\n",
    "\n",
    "                output = model(src_tensor, seq)  # (1, seq_len, vocab_size)\n",
    "                logits = output[0, -1, :]        # last token logits\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "                # Get top beam_width predictions\n",
    "                topk_log_probs, topk_ids = torch.topk(log_probs, beam_width)\n",
    "\n",
    "                for log_prob, token_id in zip(topk_log_probs, topk_ids):\n",
    "                    new_seq = torch.cat([seq, token_id.view(1, 1)], dim=1)\n",
    "                    new_score = score + log_prob.item()\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "\n",
    "            # Keep only top `beam_width` beams\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "            # If all beams have ended, stop early\n",
    "            if all(seq[0, -1].item() == eos_token_id for seq, _ in beams):\n",
    "                break\n",
    "\n",
    "    # Choose the best-scoring sequence\n",
    "    best_seq = beams[0][0][0].tolist()  # shape (seq_len,)\n",
    "    predicted_tokens = [tgt_itos[i] if i < len(tgt_itos) else \"<unk>\" for i in best_seq]\n",
    "\n",
    "    return predicted_tokens\n",
    "\n",
    "\n",
    "def evaluate(model, src_sentence, src_stoi, tgt_stoi, tgt_itos, max_len=50, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and convert source sentence using your tokenizer, here assumed pre-tokenized list of ints\n",
    "    src_tensor = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(device)  # (1, src_len)\n",
    "    \n",
    "    sos_token_id = tgt_stoi[\"<sos>\"]\n",
    "    eos_token_id = tgt_stoi[\"<eos>\"]\n",
    "    \n",
    "    tgt_tensor = torch.tensor([[sos_token_id]], dtype=torch.long).to(device)  # (1,1)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, tgt_tensor)  # (1, tgt_len, vocab_size)\n",
    "        \n",
    "        next_token_logits = output[0, -1, :]  # logits for last token\n",
    "        next_token_id = torch.argmax(next_token_logits).item()\n",
    "        \n",
    "        tgt_tensor = torch.cat([tgt_tensor, torch.tensor([[next_token_id]], device=device)], dim=1)\n",
    "        \n",
    "        if next_token_id == eos_token_id:\n",
    "            break\n",
    "    \n",
    "    predicted_ids = tgt_tensor[0].tolist()\n",
    "    predicted_tokens = [tgt_itos[i] if i < len(tgt_itos) else \"<unk>\" for i in predicted_ids]\n",
    "    \n",
    "    return predicted_tokens\n",
    "\n",
    "\n",
    "def evaluate_top_p(model, src_sentence, src_stoi, tgt_stoi, tgt_itos,max_len=50, device='cuda', temperature=0.5, top_p=0.8):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert src_sentence (list of token ids) to tensor and add batch dimension\n",
    "    src_tensor = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(device)  # shape: (1, src_len)\n",
    "\n",
    "    sos_token_id = tgt_stoi[\"<sos>\"]\n",
    "    eos_token_id = tgt_stoi[\"<eos>\"]\n",
    "\n",
    "    # Start target sequence with <sos>, shape (1,1)\n",
    "    tgt_tensor = torch.tensor([[sos_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            # Pass src and current target input to the model\n",
    "            output = model(src_tensor, tgt_tensor)  # shape: (1, tgt_len, vocab_size)\n",
    "\n",
    "        logits = output[0, -1, :] / temperature  # last token logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Top-p (nucleus) sampling\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "        sorted_probs[cumulative_probs > top_p] = 0\n",
    "        total_prob = sorted_probs.sum()\n",
    "\n",
    "        if total_prob == 0:\n",
    "            next_token_id = torch.argmax(probs).item()\n",
    "        else:\n",
    "            sorted_probs /= total_prob\n",
    "            sampled_index = torch.multinomial(sorted_probs, 1).item()\n",
    "            next_token_id = sorted_indices[sampled_index].item()\n",
    "\n",
    "        tgt_tensor = torch.cat([tgt_tensor, torch.tensor([[next_token_id]], device=device)], dim=1)\n",
    "\n",
    "        if next_token_id == eos_token_id:\n",
    "            break\n",
    "\n",
    "    predicted_ids = tgt_tensor[0].tolist()\n",
    "    predicted_tokens = [tgt_itos[i] if i < len(tgt_itos) else \"<unk>\" for i in predicted_ids]\n",
    "\n",
    "    return predicted_tokens\n",
    "\n",
    "# Use your actual tokenizer here instead of split()\n",
    "src_text = \"do you know her?\"\n",
    "src_tokens = tokenize_en(src_text)  # <-- your actual tokenizer function\n",
    "src_ids = [src_stoi.get(tok, src_stoi[\"<unk>\"]) for tok in src_tokens]\n",
    "\n",
    "result = evaluate_with_beam_search(model, src_ids, src_stoi, tgt_stoi, tgt_itos, device=device)\n",
    "print(\"Generated:\", \" \".join(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cc42eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Input: Hello, how are you?\n",
      "    Output: <sos> hallo , wie wie du du ? <eos>\n",
      "\n",
      "[2] Input: Despite the rain, they went hiking in the mountains.\n",
      "    Output: <sos> der regen gingen sie in in den berge . <eos>\n",
      "\n",
      "[3] Input: Thank you.\n",
      "    Output: <sos> danke ! ! <eos>\n",
      "\n",
      "[4] Input: The philosopher questioned the fabric of reality.\n",
      "    Output: <sos> das ergebnis die die der der der der der der der der der der träume des des problems . . <eos>\n",
      "\n",
      "[5] Input: I really like you.\n",
      "    Output: <sos> ich mag dich wirklich . du wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich wirklich . . . . . <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Despite the rain, they went hiking in the mountains.\",\n",
    "    \"Thank you.\",\n",
    "    \"The philosopher questioned the fabric of reality.\",\n",
    "    \"I really like you.\"\n",
    "]\n",
    "\n",
    "for i, src_text in enumerate(test_sentences, 1):\n",
    "    src_tokens = tokenize_en(src_text)\n",
    "    src_ids = [src_stoi.get(tok, src_stoi[\"<unk>\"]) for tok in src_tokens]\n",
    "    \n",
    "    result = evaluate(model, src_ids, src_stoi, tgt_stoi, tgt_itos, device=device)\n",
    "    print(f\"[{i}] Input: {src_text}\")\n",
    "    print(f\"    Output: {' '.join(result)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c724d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
