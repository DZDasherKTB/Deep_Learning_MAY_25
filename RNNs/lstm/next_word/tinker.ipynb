{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d49ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 140109\n",
      "First 20 tokens: ['Dog', 'bone', ',', 'stapler', ',', 'cribbage', 'board', ',', 'garlic', 'press', 'because', 'this', 'window', 'is', 'loose', 'â€”', 'lacks', 'suction', ',', 'lacks']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "df = pd.read_csv(\"data/poem.csv\", nrows=500)  \n",
    "\n",
    "poem_lines = df.iloc[:, 2].dropna().astype(str)\n",
    "poem_lines = poem_lines.apply(lambda x: x.strip())\n",
    "poem = \" \".join(poem_lines)\n",
    "\n",
    "tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", poem)\n",
    "\n",
    "# Output\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"First 20 tokens: {tokens[:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15afaba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 16491\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(tokens))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5542088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 112067, Test size: 28017\n"
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(tokens) - seq_length):\n",
    "    seq = tokens[i:i+seq_length]          # input words\n",
    "    target = tokens[i+seq_length]         # next word to predict\n",
    "    inputs.append([word2idx[w] for w in seq])\n",
    "    targets.append(word2idx[target])\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(inputs) \n",
    "y = torch.tensor(targets) \n",
    "\n",
    "dataset_size = len(X)\n",
    "split_ratio = 0.8\n",
    "split_idx = int(dataset_size * split_ratio)\n",
    "\n",
    "X_train = X[:split_idx]\n",
    "y_train = y[:split_idx]\n",
    "\n",
    "X_test = X[split_idx:]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f6dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NextWordRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)   \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True) \n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)         \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)    \n",
    "        out, _ = self.lstm(x)     \n",
    "        out = out[:, -1, :]   \n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2b02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "model = NextWordRNN(vocab_size, embed_size, hidden_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53e8b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/100, Train Loss: 7.0383, Train Accuracy: 7.67%\n",
      "Epoch 1/100, Test Loss: 6.5279, Test Accuracy: 9.43%\n",
      "Epoch 2/100, Train Loss: 6.2786, Train Accuracy: 10.47%\n",
      "Epoch 2/100, Test Loss: 5.9427, Test Accuracy: 11.90%\n",
      "Epoch 3/100, Train Loss: 5.8997, Train Accuracy: 11.80%\n",
      "Epoch 3/100, Test Loss: 5.5181, Test Accuracy: 12.98%\n",
      "Epoch 4/100, Train Loss: 5.5792, Train Accuracy: 12.66%\n",
      "Epoch 4/100, Test Loss: 5.1792, Test Accuracy: 13.64%\n",
      "Epoch 5/100, Train Loss: 5.2907, Train Accuracy: 13.42%\n",
      "Epoch 5/100, Test Loss: 4.8877, Test Accuracy: 14.71%\n",
      "Epoch 6/100, Train Loss: 5.0143, Train Accuracy: 14.25%\n",
      "Epoch 6/100, Test Loss: 4.6366, Test Accuracy: 16.28%\n",
      "Epoch 7/100, Train Loss: 4.7550, Train Accuracy: 15.53%\n",
      "Epoch 7/100, Test Loss: 4.4150, Test Accuracy: 18.40%\n",
      "Epoch 8/100, Train Loss: 4.5126, Train Accuracy: 17.49%\n",
      "Epoch 8/100, Test Loss: 4.2359, Test Accuracy: 20.81%\n",
      "Epoch 9/100, Train Loss: 4.2893, Train Accuracy: 19.99%\n",
      "Epoch 9/100, Test Loss: 4.0628, Test Accuracy: 22.88%\n",
      "Epoch 10/100, Train Loss: 4.0941, Train Accuracy: 22.35%\n",
      "Epoch 10/100, Test Loss: 3.9200, Test Accuracy: 24.88%\n",
      "Epoch 11/100, Train Loss: 3.9199, Train Accuracy: 24.62%\n",
      "Epoch 11/100, Test Loss: 3.8037, Test Accuracy: 26.17%\n",
      "Epoch 12/100, Train Loss: 3.8026, Train Accuracy: 26.26%\n",
      "Epoch 12/100, Test Loss: 3.6989, Test Accuracy: 27.53%\n",
      "Epoch 13/100, Train Loss: 3.6578, Train Accuracy: 28.35%\n",
      "Epoch 13/100, Test Loss: 3.5879, Test Accuracy: 29.14%\n",
      "Epoch 14/100, Train Loss: 3.5126, Train Accuracy: 30.53%\n",
      "Epoch 14/100, Test Loss: 3.4990, Test Accuracy: 30.36%\n",
      "Epoch 15/100, Train Loss: 3.3935, Train Accuracy: 32.33%\n",
      "Epoch 15/100, Test Loss: 3.4159, Test Accuracy: 31.51%\n",
      "Epoch 16/100, Train Loss: 3.2881, Train Accuracy: 33.88%\n",
      "Epoch 16/100, Test Loss: 3.3403, Test Accuracy: 32.44%\n",
      "Epoch 17/100, Train Loss: 3.1942, Train Accuracy: 35.43%\n",
      "Epoch 17/100, Test Loss: 3.2589, Test Accuracy: 33.89%\n",
      "Epoch 18/100, Train Loss: 3.1072, Train Accuracy: 36.77%\n",
      "Epoch 18/100, Test Loss: 3.1864, Test Accuracy: 34.87%\n",
      "Epoch 19/100, Train Loss: 3.0226, Train Accuracy: 38.11%\n",
      "Epoch 19/100, Test Loss: 3.1030, Test Accuracy: 36.14%\n",
      "Epoch 20/100, Train Loss: 2.9451, Train Accuracy: 39.40%\n",
      "Epoch 20/100, Test Loss: 3.0142, Test Accuracy: 37.87%\n",
      "Epoch 21/100, Train Loss: 2.8705, Train Accuracy: 40.58%\n",
      "Epoch 21/100, Test Loss: 2.9618, Test Accuracy: 38.62%\n",
      "Epoch 22/100, Train Loss: 2.7957, Train Accuracy: 41.82%\n",
      "Epoch 22/100, Test Loss: 2.9439, Test Accuracy: 38.60%\n",
      "Epoch 23/100, Train Loss: 2.7308, Train Accuracy: 43.04%\n",
      "Epoch 23/100, Test Loss: 2.9019, Test Accuracy: 39.08%\n",
      "Epoch 24/100, Train Loss: 2.6632, Train Accuracy: 44.05%\n",
      "Epoch 24/100, Test Loss: 2.8841, Test Accuracy: 39.44%\n",
      "Epoch 25/100, Train Loss: 2.6170, Train Accuracy: 44.89%\n",
      "Epoch 25/100, Test Loss: 2.8207, Test Accuracy: 40.25%\n",
      "Epoch 26/100, Train Loss: 2.5605, Train Accuracy: 45.93%\n",
      "Epoch 26/100, Test Loss: 2.7488, Test Accuracy: 41.70%\n",
      "Epoch 27/100, Train Loss: 2.5013, Train Accuracy: 46.97%\n",
      "Epoch 27/100, Test Loss: 2.7223, Test Accuracy: 42.00%\n",
      "Epoch 28/100, Train Loss: 2.4552, Train Accuracy: 47.76%\n",
      "Epoch 28/100, Test Loss: 2.6945, Test Accuracy: 42.60%\n",
      "Epoch 29/100, Train Loss: 2.4039, Train Accuracy: 48.60%\n",
      "Epoch 29/100, Test Loss: 2.6522, Test Accuracy: 43.15%\n",
      "Epoch 30/100, Train Loss: 2.3599, Train Accuracy: 49.34%\n",
      "Epoch 30/100, Test Loss: 2.5934, Test Accuracy: 44.32%\n",
      "Epoch 31/100, Train Loss: 2.3237, Train Accuracy: 50.05%\n",
      "Epoch 31/100, Test Loss: 2.5335, Test Accuracy: 45.18%\n",
      "Epoch 32/100, Train Loss: 2.2991, Train Accuracy: 50.36%\n",
      "Epoch 32/100, Test Loss: 2.4878, Test Accuracy: 45.94%\n",
      "Epoch 33/100, Train Loss: 2.2560, Train Accuracy: 51.26%\n",
      "Epoch 33/100, Test Loss: 2.4689, Test Accuracy: 46.14%\n",
      "Epoch 34/100, Train Loss: 2.2119, Train Accuracy: 51.97%\n",
      "Epoch 34/100, Test Loss: 2.4089, Test Accuracy: 47.32%\n",
      "Epoch 35/100, Train Loss: 2.1765, Train Accuracy: 52.69%\n",
      "Epoch 35/100, Test Loss: 2.3880, Test Accuracy: 47.69%\n",
      "Epoch 36/100, Train Loss: 2.1417, Train Accuracy: 53.40%\n",
      "Epoch 36/100, Test Loss: 2.3419, Test Accuracy: 48.25%\n",
      "Epoch 37/100, Train Loss: 2.1000, Train Accuracy: 54.08%\n",
      "Epoch 37/100, Test Loss: 2.3037, Test Accuracy: 48.85%\n",
      "Epoch 38/100, Train Loss: 2.0663, Train Accuracy: 54.78%\n",
      "Epoch 38/100, Test Loss: 2.2893, Test Accuracy: 49.27%\n",
      "Epoch 39/100, Train Loss: 2.0453, Train Accuracy: 55.09%\n",
      "Epoch 39/100, Test Loss: 2.2756, Test Accuracy: 49.51%\n",
      "Epoch 40/100, Train Loss: 2.0134, Train Accuracy: 55.65%\n",
      "Epoch 40/100, Test Loss: 2.2630, Test Accuracy: 49.59%\n",
      "Epoch 41/100, Train Loss: 1.9947, Train Accuracy: 55.92%\n",
      "Epoch 41/100, Test Loss: 2.2415, Test Accuracy: 50.41%\n",
      "Epoch 42/100, Train Loss: 1.9618, Train Accuracy: 56.64%\n",
      "Epoch 42/100, Test Loss: 2.2012, Test Accuracy: 50.76%\n",
      "Epoch 43/100, Train Loss: 1.9287, Train Accuracy: 57.21%\n",
      "Epoch 43/100, Test Loss: 2.1588, Test Accuracy: 51.55%\n",
      "Epoch 44/100, Train Loss: 1.9032, Train Accuracy: 57.70%\n",
      "Epoch 44/100, Test Loss: 2.1065, Test Accuracy: 52.54%\n",
      "Epoch 45/100, Train Loss: 1.8902, Train Accuracy: 57.84%\n",
      "Epoch 45/100, Test Loss: 2.0836, Test Accuracy: 52.88%\n",
      "Epoch 46/100, Train Loss: 1.8572, Train Accuracy: 58.51%\n",
      "Epoch 46/100, Test Loss: 2.0672, Test Accuracy: 52.98%\n",
      "Epoch 47/100, Train Loss: 1.8424, Train Accuracy: 58.78%\n",
      "Epoch 47/100, Test Loss: 2.0704, Test Accuracy: 53.25%\n",
      "Epoch 48/100, Train Loss: 1.8099, Train Accuracy: 59.47%\n",
      "Epoch 48/100, Test Loss: 2.0753, Test Accuracy: 53.40%\n",
      "Epoch 49/100, Train Loss: 1.7838, Train Accuracy: 59.89%\n",
      "Epoch 49/100, Test Loss: 2.0552, Test Accuracy: 53.62%\n",
      "Epoch 50/100, Train Loss: 1.7795, Train Accuracy: 59.86%\n",
      "Epoch 50/100, Test Loss: 2.0700, Test Accuracy: 53.28%\n",
      "Epoch 51/100, Train Loss: 1.7744, Train Accuracy: 59.95%\n",
      "Epoch 51/100, Test Loss: 2.0330, Test Accuracy: 54.02%\n",
      "Epoch 52/100, Train Loss: 1.7623, Train Accuracy: 60.21%\n",
      "Epoch 52/100, Test Loss: 2.0189, Test Accuracy: 54.35%\n",
      "Epoch 53/100, Train Loss: 1.7483, Train Accuracy: 60.50%\n",
      "Epoch 53/100, Test Loss: 1.9660, Test Accuracy: 55.48%\n",
      "Epoch 54/100, Train Loss: 1.7238, Train Accuracy: 60.96%\n",
      "Epoch 54/100, Test Loss: 1.9602, Test Accuracy: 55.57%\n",
      "Epoch 55/100, Train Loss: 1.7065, Train Accuracy: 61.25%\n",
      "Epoch 55/100, Test Loss: 1.9375, Test Accuracy: 56.03%\n",
      "Epoch 56/100, Train Loss: 1.6842, Train Accuracy: 61.72%\n",
      "Epoch 56/100, Test Loss: 1.9316, Test Accuracy: 56.27%\n",
      "Epoch 57/100, Train Loss: 1.6743, Train Accuracy: 61.92%\n",
      "Epoch 57/100, Test Loss: 1.9091, Test Accuracy: 56.74%\n",
      "Epoch 58/100, Train Loss: 1.6687, Train Accuracy: 61.86%\n",
      "Epoch 58/100, Test Loss: 1.9029, Test Accuracy: 56.55%\n",
      "Epoch 59/100, Train Loss: 1.6432, Train Accuracy: 62.53%\n",
      "Epoch 59/100, Test Loss: 1.9014, Test Accuracy: 56.60%\n",
      "Epoch 60/100, Train Loss: 1.6272, Train Accuracy: 62.78%\n",
      "Epoch 60/100, Test Loss: 1.8906, Test Accuracy: 57.00%\n",
      "Epoch 61/100, Train Loss: 1.6035, Train Accuracy: 63.40%\n",
      "Epoch 61/100, Test Loss: 1.9091, Test Accuracy: 56.33%\n",
      "Epoch 62/100, Train Loss: 1.5941, Train Accuracy: 63.52%\n",
      "Epoch 62/100, Test Loss: 1.9153, Test Accuracy: 56.16%\n",
      "Epoch 63/100, Train Loss: 1.5700, Train Accuracy: 64.04%\n",
      "Epoch 63/100, Test Loss: 1.8690, Test Accuracy: 57.10%\n",
      "Epoch 64/100, Train Loss: 1.5604, Train Accuracy: 64.22%\n",
      "Epoch 64/100, Test Loss: 1.9054, Test Accuracy: 56.43%\n",
      "Epoch 65/100, Train Loss: 1.5466, Train Accuracy: 64.54%\n",
      "Epoch 65/100, Test Loss: 1.8693, Test Accuracy: 56.92%\n",
      "Epoch 66/100, Train Loss: 1.5352, Train Accuracy: 64.74%\n",
      "Epoch 66/100, Test Loss: 1.8294, Test Accuracy: 57.97%\n",
      "Epoch 67/100, Train Loss: 1.5320, Train Accuracy: 64.70%\n",
      "Epoch 67/100, Test Loss: 1.8156, Test Accuracy: 58.27%\n",
      "Epoch 68/100, Train Loss: 1.5178, Train Accuracy: 65.03%\n",
      "Epoch 68/100, Test Loss: 1.8010, Test Accuracy: 58.50%\n",
      "Epoch 69/100, Train Loss: 1.5049, Train Accuracy: 65.26%\n",
      "Epoch 69/100, Test Loss: 1.7609, Test Accuracy: 59.47%\n",
      "Epoch 70/100, Train Loss: 1.4937, Train Accuracy: 65.55%\n",
      "Epoch 70/100, Test Loss: 1.7330, Test Accuracy: 60.18%\n",
      "Epoch 71/100, Train Loss: 1.4787, Train Accuracy: 65.84%\n",
      "Epoch 71/100, Test Loss: 1.7343, Test Accuracy: 59.95%\n",
      "Epoch 72/100, Train Loss: 1.4696, Train Accuracy: 65.90%\n",
      "Epoch 72/100, Test Loss: 1.7028, Test Accuracy: 60.46%\n",
      "Epoch 73/100, Train Loss: 1.4573, Train Accuracy: 66.08%\n",
      "Epoch 73/100, Test Loss: 1.6777, Test Accuracy: 60.85%\n",
      "Epoch 74/100, Train Loss: 1.4508, Train Accuracy: 66.37%\n",
      "Epoch 74/100, Test Loss: 1.7060, Test Accuracy: 60.39%\n",
      "Epoch 75/100, Train Loss: 1.4337, Train Accuracy: 66.65%\n",
      "Epoch 75/100, Test Loss: 1.7253, Test Accuracy: 59.95%\n",
      "Epoch 76/100, Train Loss: 1.4305, Train Accuracy: 66.73%\n",
      "Epoch 76/100, Test Loss: 1.6588, Test Accuracy: 61.48%\n",
      "Epoch 77/100, Train Loss: 1.4331, Train Accuracy: 66.64%\n",
      "Epoch 77/100, Test Loss: 1.6855, Test Accuracy: 60.90%\n",
      "Epoch 78/100, Train Loss: 1.4248, Train Accuracy: 66.88%\n",
      "Epoch 78/100, Test Loss: 1.6578, Test Accuracy: 61.70%\n",
      "Epoch 79/100, Train Loss: 1.4108, Train Accuracy: 67.00%\n",
      "Epoch 79/100, Test Loss: 1.6980, Test Accuracy: 60.71%\n",
      "Epoch 80/100, Train Loss: 1.4118, Train Accuracy: 67.08%\n",
      "Epoch 80/100, Test Loss: 1.6596, Test Accuracy: 61.59%\n",
      "Epoch 81/100, Train Loss: 1.4107, Train Accuracy: 66.89%\n",
      "Epoch 81/100, Test Loss: 1.6364, Test Accuracy: 61.94%\n",
      "Epoch 82/100, Train Loss: 1.4024, Train Accuracy: 67.10%\n",
      "Epoch 82/100, Test Loss: 1.6142, Test Accuracy: 62.48%\n",
      "Epoch 83/100, Train Loss: 1.4215, Train Accuracy: 66.65%\n",
      "Epoch 83/100, Test Loss: 1.5498, Test Accuracy: 63.63%\n",
      "Epoch 84/100, Train Loss: 1.3818, Train Accuracy: 67.76%\n",
      "Epoch 84/100, Test Loss: 1.5670, Test Accuracy: 63.58%\n",
      "Epoch 85/100, Train Loss: 1.3530, Train Accuracy: 68.24%\n",
      "Epoch 85/100, Test Loss: 1.5107, Test Accuracy: 64.62%\n",
      "Epoch 86/100, Train Loss: 1.3377, Train Accuracy: 68.62%\n",
      "Epoch 86/100, Test Loss: 1.5019, Test Accuracy: 64.76%\n",
      "Epoch 87/100, Train Loss: 1.3285, Train Accuracy: 68.86%\n",
      "Epoch 87/100, Test Loss: 1.4785, Test Accuracy: 64.96%\n",
      "Epoch 88/100, Train Loss: 1.3168, Train Accuracy: 69.09%\n",
      "Epoch 88/100, Test Loss: 1.4734, Test Accuracy: 65.49%\n",
      "Epoch 89/100, Train Loss: 1.3051, Train Accuracy: 69.33%\n",
      "Epoch 89/100, Test Loss: 1.4354, Test Accuracy: 66.21%\n",
      "Epoch 90/100, Train Loss: 1.2907, Train Accuracy: 69.71%\n",
      "Epoch 90/100, Test Loss: 1.4058, Test Accuracy: 66.75%\n",
      "Epoch 91/100, Train Loss: 1.2746, Train Accuracy: 70.08%\n",
      "Epoch 91/100, Test Loss: 1.4127, Test Accuracy: 66.90%\n",
      "Epoch 92/100, Train Loss: 1.2702, Train Accuracy: 70.05%\n",
      "Epoch 92/100, Test Loss: 1.3737, Test Accuracy: 67.41%\n",
      "Epoch 93/100, Train Loss: 1.2566, Train Accuracy: 70.41%\n",
      "Epoch 93/100, Test Loss: 1.3473, Test Accuracy: 68.03%\n",
      "Epoch 94/100, Train Loss: 1.2529, Train Accuracy: 70.31%\n",
      "Epoch 94/100, Test Loss: 1.3386, Test Accuracy: 68.30%\n",
      "Epoch 95/100, Train Loss: 1.2408, Train Accuracy: 70.62%\n",
      "Epoch 95/100, Test Loss: 1.3518, Test Accuracy: 67.81%\n",
      "Epoch 96/100, Train Loss: 1.2348, Train Accuracy: 70.63%\n",
      "Epoch 96/100, Test Loss: 1.3346, Test Accuracy: 68.17%\n",
      "Epoch 97/100, Train Loss: 1.2314, Train Accuracy: 70.79%\n",
      "Epoch 97/100, Test Loss: 1.3245, Test Accuracy: 68.55%\n",
      "Epoch 98/100, Train Loss: 1.2256, Train Accuracy: 71.03%\n",
      "Epoch 98/100, Test Loss: 1.3180, Test Accuracy: 68.67%\n",
      "Epoch 99/100, Train Loss: 1.2158, Train Accuracy: 70.99%\n",
      "Epoch 99/100, Test Loss: 1.3307, Test Accuracy: 68.08%\n",
      "Epoch 100/100, Train Loss: 1.2083, Train Accuracy: 71.24%\n",
      "Epoch 100/100, Test Loss: 1.3021, Test Accuracy: 68.99%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        xb = X[i:i+batch_size].to(device)\n",
    "        yb = y[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, dim=1) \n",
    "        correct += (predicted == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    avg_train_loss = epoch_loss / (len(X) // batch_size)\n",
    "    train_accuracy = correct / total * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # --- Testing / Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            xb = X_test[i:i+batch_size].to(device)\n",
    "            yb = y_test[i:i+batch_size].to(device)\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct_test += (predicted == yb).sum().item()\n",
    "            total_test += yb.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / (len(X_test) // batch_size)\n",
    "    test_accuracy = correct_test / total_test * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1705f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"next_word.pth\")\n",
    "import pickle\n",
    "with open(\"word2idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2idx, f)\n",
    "with open(\"idx2word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(idx2word, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50368d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, prompt, k=3):\n",
    "    model.eval()\n",
    "    prompt_tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", prompt.lower())\n",
    "    input_seq = prompt_tokens[-seq_length:]\n",
    "    input_ids = [word2idx.get(w, 0) for w in input_seq]\n",
    "    input_tensor = torch.tensor([input_ids]).to(next(model.parameters()).device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)                \n",
    "        probs = torch.softmax(output, dim=1)         # probabilities\n",
    "        top_probs, top_indices = torch.topk(probs, k)\n",
    "\n",
    "        predictions = [idx2word[idx.item()] for idx in top_indices[0]]\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c1c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c3bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
