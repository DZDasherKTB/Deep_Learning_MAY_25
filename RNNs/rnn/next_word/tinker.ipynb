{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d49ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 262927\n",
      "First 20 tokens: ['first', 'citizen', ':', 'before', 'we', 'proceed', 'any', 'further', ',', 'hear', 'me', 'speak', '.', 'all', ':', 'speak', ',', 'speak', '.', 'first']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Read Shakespeare text file\n",
    "with open(\"data/input.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "# Tokenize with regex: words + punctuation\n",
    "tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", text)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"First 20 tokens: {tokens[:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15afaba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 11466\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(tokens))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5542088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 210337, Test size: 52585\n"
     ]
    }
   ],
   "source": [
    "seq_length = 5\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(tokens) - seq_length):\n",
    "    seq = tokens[i:i+seq_length]          # input words\n",
    "    target = tokens[i+seq_length]         # next word to predict\n",
    "    inputs.append([word2idx[w] for w in seq])\n",
    "    targets.append(word2idx[target])\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(inputs)   # Shape: (num_sequences, seq_length)\n",
    "y = torch.tensor(targets)  # Shape: (num_sequences,)\n",
    "\n",
    "dataset_size = len(X)\n",
    "split_ratio = 0.8\n",
    "split_idx = int(dataset_size * split_ratio)\n",
    "\n",
    "X_train = X[:split_idx]\n",
    "y_train = y[:split_idx]\n",
    "\n",
    "X_test = X[split_idx:]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f6dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NextWordRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)   # Turns word IDs into vectors\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)  # RNN layer\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)            # Output layer (predict vocab logits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)            # (batch_size, seq_length, embed_size)\n",
    "        out, _ = self.rnn(x)             # (batch_size, seq_length, hidden_size)\n",
    "        out = out[:, -1, :]              # Take output from last time step\n",
    "        out = self.fc(out)               # (batch_size, vocab_size)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d2b02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "model = NextWordRNN(vocab_size, embed_size, hidden_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e53e8b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/100, Train Loss: 3.1484, Train Accuracy: 34.30%\n",
      "Epoch 1/100, Test Loss: 3.5671, Test Accuracy: 27.97%\n",
      "Epoch 2/100, Train Loss: 3.1299, Train Accuracy: 34.61%\n",
      "Epoch 2/100, Test Loss: 3.5447, Test Accuracy: 28.34%\n",
      "Epoch 3/100, Train Loss: 3.1142, Train Accuracy: 34.87%\n",
      "Epoch 3/100, Test Loss: 3.5194, Test Accuracy: 28.62%\n",
      "Epoch 4/100, Train Loss: 3.1002, Train Accuracy: 35.10%\n",
      "Epoch 4/100, Test Loss: 3.5073, Test Accuracy: 28.79%\n",
      "Epoch 5/100, Train Loss: 3.0867, Train Accuracy: 35.38%\n",
      "Epoch 5/100, Test Loss: 3.4963, Test Accuracy: 28.98%\n",
      "Epoch 6/100, Train Loss: 3.0756, Train Accuracy: 35.53%\n",
      "Epoch 6/100, Test Loss: 3.4882, Test Accuracy: 29.22%\n",
      "Epoch 7/100, Train Loss: 3.0640, Train Accuracy: 35.70%\n",
      "Epoch 7/100, Test Loss: 3.4801, Test Accuracy: 29.44%\n",
      "Epoch 8/100, Train Loss: 3.0553, Train Accuracy: 35.88%\n",
      "Epoch 8/100, Test Loss: 3.4710, Test Accuracy: 29.61%\n",
      "Epoch 9/100, Train Loss: 3.0484, Train Accuracy: 35.94%\n",
      "Epoch 9/100, Test Loss: 3.4754, Test Accuracy: 29.57%\n",
      "Epoch 10/100, Train Loss: 3.0400, Train Accuracy: 36.06%\n",
      "Epoch 10/100, Test Loss: 3.4762, Test Accuracy: 29.53%\n",
      "Epoch 11/100, Train Loss: 3.0313, Train Accuracy: 36.23%\n",
      "Epoch 11/100, Test Loss: 3.4684, Test Accuracy: 29.63%\n",
      "Epoch 12/100, Train Loss: 3.0229, Train Accuracy: 36.37%\n",
      "Epoch 12/100, Test Loss: 3.4707, Test Accuracy: 29.61%\n",
      "Epoch 13/100, Train Loss: 3.0178, Train Accuracy: 36.44%\n",
      "Epoch 13/100, Test Loss: 3.4613, Test Accuracy: 29.75%\n",
      "Epoch 14/100, Train Loss: 3.0097, Train Accuracy: 36.49%\n",
      "Epoch 14/100, Test Loss: 3.4563, Test Accuracy: 29.94%\n",
      "Epoch 15/100, Train Loss: 3.0042, Train Accuracy: 36.60%\n",
      "Epoch 15/100, Test Loss: 3.4543, Test Accuracy: 29.83%\n",
      "Epoch 16/100, Train Loss: 2.9992, Train Accuracy: 36.69%\n",
      "Epoch 16/100, Test Loss: 3.4539, Test Accuracy: 30.01%\n",
      "Epoch 17/100, Train Loss: 2.9958, Train Accuracy: 36.72%\n",
      "Epoch 17/100, Test Loss: 3.4578, Test Accuracy: 29.87%\n",
      "Epoch 18/100, Train Loss: 2.9912, Train Accuracy: 36.78%\n",
      "Epoch 18/100, Test Loss: 3.4351, Test Accuracy: 30.20%\n",
      "Epoch 19/100, Train Loss: 2.9859, Train Accuracy: 36.83%\n",
      "Epoch 19/100, Test Loss: 3.4243, Test Accuracy: 30.29%\n",
      "Epoch 20/100, Train Loss: 2.9810, Train Accuracy: 36.91%\n",
      "Epoch 20/100, Test Loss: 3.4260, Test Accuracy: 30.22%\n",
      "Epoch 21/100, Train Loss: 2.9771, Train Accuracy: 36.96%\n",
      "Epoch 21/100, Test Loss: 3.4311, Test Accuracy: 30.22%\n",
      "Epoch 22/100, Train Loss: 2.9746, Train Accuracy: 37.01%\n",
      "Epoch 22/100, Test Loss: 3.4271, Test Accuracy: 30.35%\n",
      "Epoch 23/100, Train Loss: 2.9704, Train Accuracy: 37.07%\n",
      "Epoch 23/100, Test Loss: 3.4231, Test Accuracy: 30.28%\n",
      "Epoch 24/100, Train Loss: 2.9660, Train Accuracy: 37.04%\n",
      "Epoch 24/100, Test Loss: 3.4216, Test Accuracy: 30.39%\n",
      "Epoch 25/100, Train Loss: 2.9627, Train Accuracy: 37.06%\n",
      "Epoch 25/100, Test Loss: 3.4257, Test Accuracy: 30.22%\n",
      "Epoch 26/100, Train Loss: 2.9581, Train Accuracy: 37.20%\n",
      "Epoch 26/100, Test Loss: 3.4205, Test Accuracy: 30.28%\n",
      "Epoch 27/100, Train Loss: 2.9543, Train Accuracy: 37.19%\n",
      "Epoch 27/100, Test Loss: 3.4108, Test Accuracy: 30.36%\n",
      "Epoch 28/100, Train Loss: 2.9502, Train Accuracy: 37.27%\n",
      "Epoch 28/100, Test Loss: 3.4092, Test Accuracy: 30.35%\n",
      "Epoch 29/100, Train Loss: 2.9431, Train Accuracy: 37.37%\n",
      "Epoch 29/100, Test Loss: 3.4054, Test Accuracy: 30.32%\n",
      "Epoch 30/100, Train Loss: 2.9361, Train Accuracy: 37.52%\n",
      "Epoch 30/100, Test Loss: 3.4140, Test Accuracy: 30.43%\n",
      "Epoch 31/100, Train Loss: 2.9327, Train Accuracy: 37.58%\n",
      "Epoch 31/100, Test Loss: 3.3959, Test Accuracy: 30.43%\n",
      "Epoch 32/100, Train Loss: 2.9279, Train Accuracy: 37.63%\n",
      "Epoch 32/100, Test Loss: 3.3850, Test Accuracy: 30.67%\n",
      "Epoch 33/100, Train Loss: 2.9245, Train Accuracy: 37.70%\n",
      "Epoch 33/100, Test Loss: 3.3742, Test Accuracy: 30.85%\n",
      "Epoch 34/100, Train Loss: 2.9198, Train Accuracy: 37.74%\n",
      "Epoch 34/100, Test Loss: 3.3836, Test Accuracy: 30.78%\n",
      "Epoch 35/100, Train Loss: 2.9161, Train Accuracy: 37.82%\n",
      "Epoch 35/100, Test Loss: 3.3684, Test Accuracy: 30.99%\n",
      "Epoch 36/100, Train Loss: 2.9117, Train Accuracy: 37.87%\n",
      "Epoch 36/100, Test Loss: 3.3739, Test Accuracy: 30.89%\n",
      "Epoch 37/100, Train Loss: 2.9023, Train Accuracy: 38.09%\n",
      "Epoch 37/100, Test Loss: 3.3641, Test Accuracy: 30.89%\n",
      "Epoch 38/100, Train Loss: 2.8992, Train Accuracy: 38.09%\n",
      "Epoch 38/100, Test Loss: 3.3615, Test Accuracy: 30.97%\n",
      "Epoch 39/100, Train Loss: 2.8947, Train Accuracy: 38.16%\n",
      "Epoch 39/100, Test Loss: 3.3451, Test Accuracy: 31.22%\n",
      "Epoch 40/100, Train Loss: 2.8887, Train Accuracy: 38.28%\n",
      "Epoch 40/100, Test Loss: 3.3387, Test Accuracy: 31.31%\n",
      "Epoch 41/100, Train Loss: 2.8820, Train Accuracy: 38.37%\n",
      "Epoch 41/100, Test Loss: 3.3334, Test Accuracy: 31.38%\n",
      "Epoch 42/100, Train Loss: 2.8777, Train Accuracy: 38.50%\n",
      "Epoch 42/100, Test Loss: 3.3302, Test Accuracy: 31.53%\n",
      "Epoch 43/100, Train Loss: 2.8735, Train Accuracy: 38.50%\n",
      "Epoch 43/100, Test Loss: 3.3251, Test Accuracy: 31.48%\n",
      "Epoch 44/100, Train Loss: 2.8725, Train Accuracy: 38.49%\n",
      "Epoch 44/100, Test Loss: 3.3168, Test Accuracy: 31.59%\n",
      "Epoch 45/100, Train Loss: 2.8659, Train Accuracy: 38.69%\n",
      "Epoch 45/100, Test Loss: 3.3045, Test Accuracy: 31.86%\n",
      "Epoch 46/100, Train Loss: 2.8605, Train Accuracy: 38.75%\n",
      "Epoch 46/100, Test Loss: 3.2993, Test Accuracy: 31.88%\n",
      "Epoch 47/100, Train Loss: 2.8545, Train Accuracy: 38.79%\n",
      "Epoch 47/100, Test Loss: 3.3012, Test Accuracy: 32.01%\n",
      "Epoch 48/100, Train Loss: 2.8520, Train Accuracy: 38.90%\n",
      "Epoch 48/100, Test Loss: 3.3003, Test Accuracy: 31.94%\n",
      "Epoch 49/100, Train Loss: 2.8451, Train Accuracy: 39.05%\n",
      "Epoch 49/100, Test Loss: 3.2946, Test Accuracy: 32.00%\n",
      "Epoch 50/100, Train Loss: 2.8405, Train Accuracy: 39.07%\n",
      "Epoch 50/100, Test Loss: 3.2824, Test Accuracy: 32.23%\n",
      "Epoch 51/100, Train Loss: 2.8377, Train Accuracy: 39.15%\n",
      "Epoch 51/100, Test Loss: 3.2794, Test Accuracy: 32.12%\n",
      "Epoch 52/100, Train Loss: 2.8362, Train Accuracy: 39.10%\n",
      "Epoch 52/100, Test Loss: 3.2670, Test Accuracy: 32.42%\n",
      "Epoch 53/100, Train Loss: 2.8331, Train Accuracy: 39.09%\n",
      "Epoch 53/100, Test Loss: 3.2721, Test Accuracy: 32.31%\n",
      "Epoch 54/100, Train Loss: 2.8265, Train Accuracy: 39.29%\n",
      "Epoch 54/100, Test Loss: 3.2630, Test Accuracy: 32.53%\n",
      "Epoch 55/100, Train Loss: 2.8235, Train Accuracy: 39.35%\n",
      "Epoch 55/100, Test Loss: 3.2649, Test Accuracy: 32.37%\n",
      "Epoch 56/100, Train Loss: 2.8237, Train Accuracy: 39.36%\n",
      "Epoch 56/100, Test Loss: 3.2558, Test Accuracy: 32.56%\n",
      "Epoch 57/100, Train Loss: 2.8208, Train Accuracy: 39.40%\n",
      "Epoch 57/100, Test Loss: 3.2405, Test Accuracy: 32.75%\n",
      "Epoch 58/100, Train Loss: 2.8153, Train Accuracy: 39.47%\n",
      "Epoch 58/100, Test Loss: 3.2539, Test Accuracy: 32.59%\n",
      "Epoch 59/100, Train Loss: 2.8113, Train Accuracy: 39.56%\n",
      "Epoch 59/100, Test Loss: 3.2338, Test Accuracy: 32.83%\n",
      "Epoch 60/100, Train Loss: 2.8110, Train Accuracy: 39.53%\n",
      "Epoch 60/100, Test Loss: 3.2346, Test Accuracy: 32.82%\n",
      "Epoch 61/100, Train Loss: 2.8067, Train Accuracy: 39.51%\n",
      "Epoch 61/100, Test Loss: 3.2354, Test Accuracy: 32.77%\n",
      "Epoch 62/100, Train Loss: 2.8037, Train Accuracy: 39.62%\n",
      "Epoch 62/100, Test Loss: 3.2420, Test Accuracy: 32.74%\n",
      "Epoch 63/100, Train Loss: 2.7980, Train Accuracy: 39.69%\n",
      "Epoch 63/100, Test Loss: 3.2283, Test Accuracy: 33.05%\n",
      "Epoch 64/100, Train Loss: 2.7978, Train Accuracy: 39.73%\n",
      "Epoch 64/100, Test Loss: 3.2167, Test Accuracy: 33.22%\n",
      "Epoch 65/100, Train Loss: 2.7937, Train Accuracy: 39.70%\n",
      "Epoch 65/100, Test Loss: 3.2229, Test Accuracy: 33.05%\n",
      "Epoch 66/100, Train Loss: 2.7931, Train Accuracy: 39.75%\n",
      "Epoch 66/100, Test Loss: 3.2294, Test Accuracy: 33.06%\n",
      "Epoch 67/100, Train Loss: 2.7850, Train Accuracy: 39.98%\n",
      "Epoch 67/100, Test Loss: 3.2120, Test Accuracy: 33.10%\n",
      "Epoch 68/100, Train Loss: 2.7844, Train Accuracy: 39.92%\n",
      "Epoch 68/100, Test Loss: 3.2064, Test Accuracy: 33.26%\n",
      "Epoch 69/100, Train Loss: 2.7809, Train Accuracy: 40.00%\n",
      "Epoch 69/100, Test Loss: 3.2128, Test Accuracy: 33.31%\n",
      "Epoch 70/100, Train Loss: 2.7801, Train Accuracy: 39.94%\n",
      "Epoch 70/100, Test Loss: 3.2085, Test Accuracy: 33.17%\n",
      "Epoch 71/100, Train Loss: 2.7776, Train Accuracy: 40.06%\n",
      "Epoch 71/100, Test Loss: 3.1953, Test Accuracy: 33.50%\n",
      "Epoch 72/100, Train Loss: 2.7748, Train Accuracy: 40.12%\n",
      "Epoch 72/100, Test Loss: 3.1986, Test Accuracy: 33.42%\n",
      "Epoch 73/100, Train Loss: 2.7718, Train Accuracy: 40.12%\n",
      "Epoch 73/100, Test Loss: 3.1912, Test Accuracy: 33.49%\n",
      "Epoch 74/100, Train Loss: 2.7688, Train Accuracy: 40.21%\n",
      "Epoch 74/100, Test Loss: 3.1699, Test Accuracy: 33.98%\n",
      "Epoch 75/100, Train Loss: 2.7652, Train Accuracy: 40.32%\n",
      "Epoch 75/100, Test Loss: 3.1742, Test Accuracy: 33.88%\n",
      "Epoch 76/100, Train Loss: 2.7638, Train Accuracy: 40.26%\n",
      "Epoch 76/100, Test Loss: 3.1558, Test Accuracy: 34.02%\n",
      "Epoch 77/100, Train Loss: 2.7609, Train Accuracy: 40.34%\n",
      "Epoch 77/100, Test Loss: 3.1727, Test Accuracy: 33.81%\n",
      "Epoch 78/100, Train Loss: 2.7589, Train Accuracy: 40.37%\n",
      "Epoch 78/100, Test Loss: 3.1482, Test Accuracy: 34.21%\n",
      "Epoch 79/100, Train Loss: 2.7522, Train Accuracy: 40.55%\n",
      "Epoch 79/100, Test Loss: 3.1466, Test Accuracy: 34.14%\n",
      "Epoch 80/100, Train Loss: 2.7520, Train Accuracy: 40.50%\n",
      "Epoch 80/100, Test Loss: 3.1516, Test Accuracy: 34.18%\n",
      "Epoch 81/100, Train Loss: 2.7518, Train Accuracy: 40.50%\n",
      "Epoch 81/100, Test Loss: 3.1435, Test Accuracy: 34.18%\n",
      "Epoch 82/100, Train Loss: 2.7489, Train Accuracy: 40.52%\n",
      "Epoch 82/100, Test Loss: 3.1339, Test Accuracy: 34.41%\n",
      "Epoch 83/100, Train Loss: 2.7495, Train Accuracy: 40.52%\n",
      "Epoch 83/100, Test Loss: 3.1340, Test Accuracy: 34.39%\n",
      "Epoch 84/100, Train Loss: 2.7440, Train Accuracy: 40.62%\n",
      "Epoch 84/100, Test Loss: 3.1149, Test Accuracy: 34.48%\n",
      "Epoch 85/100, Train Loss: 2.7430, Train Accuracy: 40.59%\n",
      "Epoch 85/100, Test Loss: 3.1169, Test Accuracy: 34.45%\n",
      "Epoch 86/100, Train Loss: 2.7406, Train Accuracy: 40.69%\n",
      "Epoch 86/100, Test Loss: 3.1015, Test Accuracy: 34.71%\n",
      "Epoch 87/100, Train Loss: 2.7378, Train Accuracy: 40.76%\n",
      "Epoch 87/100, Test Loss: 3.1090, Test Accuracy: 34.72%\n",
      "Epoch 88/100, Train Loss: 2.7401, Train Accuracy: 40.67%\n",
      "Epoch 88/100, Test Loss: 3.1264, Test Accuracy: 34.56%\n",
      "Epoch 89/100, Train Loss: 2.7349, Train Accuracy: 40.77%\n",
      "Epoch 89/100, Test Loss: 3.1159, Test Accuracy: 34.44%\n",
      "Epoch 90/100, Train Loss: 2.7296, Train Accuracy: 40.88%\n",
      "Epoch 90/100, Test Loss: 3.0964, Test Accuracy: 34.88%\n",
      "Epoch 91/100, Train Loss: 2.7278, Train Accuracy: 40.84%\n",
      "Epoch 91/100, Test Loss: 3.0984, Test Accuracy: 34.94%\n",
      "Epoch 92/100, Train Loss: 2.7289, Train Accuracy: 40.93%\n",
      "Epoch 92/100, Test Loss: 3.0947, Test Accuracy: 35.01%\n",
      "Epoch 93/100, Train Loss: 2.7233, Train Accuracy: 40.96%\n",
      "Epoch 93/100, Test Loss: 3.0761, Test Accuracy: 35.03%\n",
      "Epoch 94/100, Train Loss: 2.7219, Train Accuracy: 41.08%\n",
      "Epoch 94/100, Test Loss: 3.0796, Test Accuracy: 35.21%\n",
      "Epoch 95/100, Train Loss: 2.7198, Train Accuracy: 41.06%\n",
      "Epoch 95/100, Test Loss: 3.0839, Test Accuracy: 35.25%\n",
      "Epoch 96/100, Train Loss: 2.7181, Train Accuracy: 41.09%\n",
      "Epoch 96/100, Test Loss: 3.0718, Test Accuracy: 35.37%\n",
      "Epoch 97/100, Train Loss: 2.7196, Train Accuracy: 41.04%\n",
      "Epoch 97/100, Test Loss: 3.0587, Test Accuracy: 35.58%\n",
      "Epoch 98/100, Train Loss: 2.7141, Train Accuracy: 41.09%\n",
      "Epoch 98/100, Test Loss: 3.0557, Test Accuracy: 35.64%\n",
      "Epoch 99/100, Train Loss: 2.7105, Train Accuracy: 41.23%\n",
      "Epoch 99/100, Test Loss: 3.0481, Test Accuracy: 35.78%\n",
      "Epoch 100/100, Train Loss: 2.7109, Train Accuracy: 41.18%\n",
      "Epoch 100/100, Test Loss: 3.0521, Test Accuracy: 35.68%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Assuming you have separate test sets X_test, y_test already prepared and on device:\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        xb = X[i:i+batch_size].to(device)\n",
    "        yb = y[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)  # Shape: (batch_size, vocab_size) or (batch_size, num_classes)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, dim=1)   # predicted classes\n",
    "        correct += (predicted == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    avg_train_loss = epoch_loss / (len(X) // batch_size)\n",
    "    train_accuracy = correct / total * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # --- Testing / Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            xb = X_test[i:i+batch_size].to(device)\n",
    "            yb = y_test[i:i+batch_size].to(device)\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct_test += (predicted == yb).sum().item()\n",
    "            total_test += yb.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / (len(X_test) // batch_size)\n",
    "    test_accuracy = correct_test / total_test * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c1705f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"next_word.pth\")\n",
    "import pickle\n",
    "with open(\"word2idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2idx, f)\n",
    "with open(\"idx2word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(idx2word, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50368d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, prompt, k=3):\n",
    "    model.eval()\n",
    "    prompt_tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", prompt.lower())\n",
    "    input_seq = prompt_tokens[-seq_length:]\n",
    "    input_ids = [word2idx.get(w, 0) for w in input_seq]\n",
    "    input_tensor = torch.tensor([input_ids]).to(next(model.parameters()).device)  # move to model's device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)                 # logits over vocab\n",
    "        probs = torch.softmax(output, dim=1)         # probabilities\n",
    "        top_probs, top_indices = torch.topk(probs, k)\n",
    "\n",
    "        predictions = [idx2word[idx.item()] for idx in top_indices[0]]\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"welcome to the world of AI!\")    \n",
    "\n",
    "prompt = \"to be or not to be\"\n",
    "predictions = predict_next_word(model, prompt)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"Predicted next words:\", predictions)\n",
    "\n",
    "for i in range(5):\n",
    "    prompt = prompt+\" \"+input(\"Enter a prompt: \")\n",
    "    if prompt.lower() == \"exit\":\n",
    "        break\n",
    "    predictions = predict_next_word(model, prompt)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"Predicted next words:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c3bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
