{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19d49ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 307771\n",
      "First 20 tokens: ['tweet', 'As', 'a', 'woman', 'you', 'shouldnt', 'complain', 'about', 'cleaning', 'up', 'your', 'house', 'as', 'a', 'man', 'you', 'should', 'always', 'take', 'the', 'trash', 'out', 'boy', 'dats', 'coldtyga', 'dwn', 'bad', 'for', 'cuffin', 'dat', 'hoe', 'in', 'the', '1st', 'place', 'You', 'ever', 'fuck', 'a', 'bitch', 'and', 'she', 'start', 'to', 'cry', 'You', 'be', 'confused', 'as', 'shit']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "\n",
    "def clean_tweet(text):\n",
    "    text = html.unescape(text)  # Convert HTML entities to characters\n",
    "    text = re.sub(r'&#\\d+;', '', text)  # Remove emoji codes\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'^.*?:', '', text, count=1)  # Remove everything before first colon\n",
    "    text = re.sub(r'http\\S+', '', text)  # Optional: remove URLs\n",
    "    text = re.sub(r'#\\w+', '', text)  # Optional: remove hashtags\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "# Load 50,000 rows from column index 6 (tweet)\n",
    "df = pd.read_csv(\"data/labeled_data.csv\", usecols=[6], names=[\"tweet\"], header=None)\n",
    "\n",
    "# Clean each tweet\n",
    "df[\"cleaned\"] = df[\"tweet\"].astype(str).apply(clean_tweet)\n",
    "\n",
    "# Join all cleaned tweets into one paragraph\n",
    "paragraph = \" \".join(df[\"cleaned\"].tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize with regex: words + punctuation\n",
    "tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\",paragraph)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"First 20 tokens: {tokens[:50]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15afaba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 25806\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(tokens))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5542088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 246212, Test size: 61554\n"
     ]
    }
   ],
   "source": [
    "seq_length = 5\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(tokens) - seq_length):\n",
    "    seq = tokens[i:i+seq_length]          # input words\n",
    "    target = tokens[i+seq_length]         # next word to predict\n",
    "    inputs.append([word2idx[w] for w in seq])\n",
    "    targets.append(word2idx[target])\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(inputs)   # Shape: (num_sequences, seq_length)\n",
    "y = torch.tensor(targets)  # Shape: (num_sequences,)\n",
    "\n",
    "dataset_size = len(X)\n",
    "split_ratio = 0.8\n",
    "split_idx = int(dataset_size * split_ratio)\n",
    "\n",
    "X_train = X[:split_idx]\n",
    "y_train = y[:split_idx]\n",
    "\n",
    "X_test = X[split_idx:]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6f6dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NextWordRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)   # Turns word IDs into vectors\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)  # RNN layer\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)            # Output layer (predict vocab logits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)            # (batch_size, seq_length, embed_size)\n",
    "        out, _ = self.rnn(x)             # (batch_size, seq_length, hidden_size)\n",
    "        out = out[:, -1, :]              # Take output from last time step\n",
    "        out = self.fc(out)               # (batch_size, vocab_size)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2b02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "model = NextWordRNN(vocab_size, embed_size, hidden_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e53e8b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/100, Train Loss: 7.1379, Train Accuracy: 6.64%\n",
      "Epoch 1/100, Test Loss: 6.4390, Test Accuracy: 8.57%\n",
      "Epoch 2/100, Train Loss: 6.3563, Train Accuracy: 9.06%\n",
      "Epoch 2/100, Test Loss: 5.8369, Test Accuracy: 9.97%\n",
      "Epoch 3/100, Train Loss: 5.9460, Train Accuracy: 10.19%\n",
      "Epoch 3/100, Test Loss: 5.4105, Test Accuracy: 11.23%\n",
      "Epoch 4/100, Train Loss: 5.6212, Train Accuracy: 11.13%\n",
      "Epoch 4/100, Test Loss: 5.0673, Test Accuracy: 13.65%\n",
      "Epoch 5/100, Train Loss: 5.3540, Train Accuracy: 12.40%\n",
      "Epoch 5/100, Test Loss: 4.7974, Test Accuracy: 16.68%\n",
      "Epoch 6/100, Train Loss: 5.1164, Train Accuracy: 14.31%\n",
      "Epoch 6/100, Test Loss: 4.5866, Test Accuracy: 18.92%\n",
      "Epoch 7/100, Train Loss: 4.9018, Train Accuracy: 16.24%\n",
      "Epoch 7/100, Test Loss: 4.4106, Test Accuracy: 20.85%\n",
      "Epoch 8/100, Train Loss: 4.7040, Train Accuracy: 18.14%\n",
      "Epoch 8/100, Test Loss: 4.2587, Test Accuracy: 22.73%\n",
      "Epoch 9/100, Train Loss: 4.5306, Train Accuracy: 20.10%\n",
      "Epoch 9/100, Test Loss: 4.1259, Test Accuracy: 24.41%\n",
      "Epoch 10/100, Train Loss: 4.3749, Train Accuracy: 21.93%\n",
      "Epoch 10/100, Test Loss: 4.0097, Test Accuracy: 25.96%\n",
      "Epoch 11/100, Train Loss: 4.2337, Train Accuracy: 23.71%\n",
      "Epoch 11/100, Test Loss: 3.9075, Test Accuracy: 27.38%\n",
      "Epoch 12/100, Train Loss: 4.1048, Train Accuracy: 25.31%\n",
      "Epoch 12/100, Test Loss: 3.8185, Test Accuracy: 28.57%\n",
      "Epoch 13/100, Train Loss: 3.9865, Train Accuracy: 26.86%\n",
      "Epoch 13/100, Test Loss: 3.7420, Test Accuracy: 29.54%\n",
      "Epoch 14/100, Train Loss: 3.8777, Train Accuracy: 28.28%\n",
      "Epoch 14/100, Test Loss: 3.6778, Test Accuracy: 30.48%\n",
      "Epoch 15/100, Train Loss: 3.7771, Train Accuracy: 29.73%\n",
      "Epoch 15/100, Test Loss: 3.6242, Test Accuracy: 31.08%\n",
      "Epoch 16/100, Train Loss: 3.6841, Train Accuracy: 31.07%\n",
      "Epoch 16/100, Test Loss: 3.5803, Test Accuracy: 31.65%\n",
      "Epoch 17/100, Train Loss: 3.5989, Train Accuracy: 32.31%\n",
      "Epoch 17/100, Test Loss: 3.5416, Test Accuracy: 32.10%\n",
      "Epoch 18/100, Train Loss: 3.5209, Train Accuracy: 33.50%\n",
      "Epoch 18/100, Test Loss: 3.5062, Test Accuracy: 32.70%\n",
      "Epoch 19/100, Train Loss: 3.4494, Train Accuracy: 34.64%\n",
      "Epoch 19/100, Test Loss: 3.4722, Test Accuracy: 33.15%\n",
      "Epoch 20/100, Train Loss: 3.3840, Train Accuracy: 35.65%\n",
      "Epoch 20/100, Test Loss: 3.4437, Test Accuracy: 33.59%\n",
      "Epoch 21/100, Train Loss: 3.3248, Train Accuracy: 36.67%\n",
      "Epoch 21/100, Test Loss: 3.4164, Test Accuracy: 34.11%\n",
      "Epoch 22/100, Train Loss: 3.2720, Train Accuracy: 37.54%\n",
      "Epoch 22/100, Test Loss: 3.3977, Test Accuracy: 34.27%\n",
      "Epoch 23/100, Train Loss: 3.2244, Train Accuracy: 38.29%\n",
      "Epoch 23/100, Test Loss: 3.3818, Test Accuracy: 34.50%\n",
      "Epoch 24/100, Train Loss: 3.1820, Train Accuracy: 38.95%\n",
      "Epoch 24/100, Test Loss: 3.3651, Test Accuracy: 34.77%\n",
      "Epoch 25/100, Train Loss: 3.1427, Train Accuracy: 39.66%\n",
      "Epoch 25/100, Test Loss: 3.3541, Test Accuracy: 34.81%\n",
      "Epoch 26/100, Train Loss: 3.1084, Train Accuracy: 40.17%\n",
      "Epoch 26/100, Test Loss: 3.3426, Test Accuracy: 34.98%\n",
      "Epoch 27/100, Train Loss: 3.0780, Train Accuracy: 40.69%\n",
      "Epoch 27/100, Test Loss: 3.3313, Test Accuracy: 35.14%\n",
      "Epoch 28/100, Train Loss: 3.0505, Train Accuracy: 41.15%\n",
      "Epoch 28/100, Test Loss: 3.3275, Test Accuracy: 35.17%\n",
      "Epoch 29/100, Train Loss: 3.0260, Train Accuracy: 41.54%\n",
      "Epoch 29/100, Test Loss: 3.3154, Test Accuracy: 35.32%\n",
      "Epoch 30/100, Train Loss: 3.0027, Train Accuracy: 41.86%\n",
      "Epoch 30/100, Test Loss: 3.3092, Test Accuracy: 35.40%\n",
      "Epoch 31/100, Train Loss: 2.9812, Train Accuracy: 42.25%\n",
      "Epoch 31/100, Test Loss: 3.3012, Test Accuracy: 35.45%\n",
      "Epoch 32/100, Train Loss: 2.9632, Train Accuracy: 42.55%\n",
      "Epoch 32/100, Test Loss: 3.3027, Test Accuracy: 35.46%\n",
      "Epoch 33/100, Train Loss: 2.9465, Train Accuracy: 42.85%\n",
      "Epoch 33/100, Test Loss: 3.3027, Test Accuracy: 35.48%\n",
      "Epoch 34/100, Train Loss: 2.9316, Train Accuracy: 43.05%\n",
      "Epoch 34/100, Test Loss: 3.2976, Test Accuracy: 35.54%\n",
      "Epoch 35/100, Train Loss: 2.9185, Train Accuracy: 43.31%\n",
      "Epoch 35/100, Test Loss: 3.2967, Test Accuracy: 35.56%\n",
      "Epoch 36/100, Train Loss: 2.9068, Train Accuracy: 43.56%\n",
      "Epoch 36/100, Test Loss: 3.2831, Test Accuracy: 35.97%\n",
      "Epoch 37/100, Train Loss: 2.8935, Train Accuracy: 43.70%\n",
      "Epoch 37/100, Test Loss: 3.2836, Test Accuracy: 35.95%\n",
      "Epoch 38/100, Train Loss: 2.8828, Train Accuracy: 43.91%\n",
      "Epoch 38/100, Test Loss: 3.2895, Test Accuracy: 35.71%\n",
      "Epoch 39/100, Train Loss: 2.8736, Train Accuracy: 44.02%\n",
      "Epoch 39/100, Test Loss: 3.2779, Test Accuracy: 36.06%\n",
      "Epoch 40/100, Train Loss: 2.8641, Train Accuracy: 44.17%\n",
      "Epoch 40/100, Test Loss: 3.2693, Test Accuracy: 36.05%\n",
      "Epoch 41/100, Train Loss: 2.8549, Train Accuracy: 44.26%\n",
      "Epoch 41/100, Test Loss: 3.2744, Test Accuracy: 35.93%\n",
      "Epoch 42/100, Train Loss: 2.8465, Train Accuracy: 44.44%\n",
      "Epoch 42/100, Test Loss: 3.2644, Test Accuracy: 36.23%\n",
      "Epoch 43/100, Train Loss: 2.8405, Train Accuracy: 44.50%\n",
      "Epoch 43/100, Test Loss: 3.2685, Test Accuracy: 36.09%\n",
      "Epoch 44/100, Train Loss: 2.8332, Train Accuracy: 44.60%\n",
      "Epoch 44/100, Test Loss: 3.2646, Test Accuracy: 36.22%\n",
      "Epoch 45/100, Train Loss: 2.8267, Train Accuracy: 44.67%\n",
      "Epoch 45/100, Test Loss: 3.2596, Test Accuracy: 36.27%\n",
      "Epoch 46/100, Train Loss: 2.8205, Train Accuracy: 44.83%\n",
      "Epoch 46/100, Test Loss: 3.2478, Test Accuracy: 36.43%\n",
      "Epoch 47/100, Train Loss: 2.8135, Train Accuracy: 44.91%\n",
      "Epoch 47/100, Test Loss: 3.2411, Test Accuracy: 36.55%\n",
      "Epoch 48/100, Train Loss: 2.8064, Train Accuracy: 44.96%\n",
      "Epoch 48/100, Test Loss: 3.2241, Test Accuracy: 36.76%\n",
      "Epoch 49/100, Train Loss: 2.7998, Train Accuracy: 45.08%\n",
      "Epoch 49/100, Test Loss: 3.2235, Test Accuracy: 36.76%\n",
      "Epoch 50/100, Train Loss: 2.7940, Train Accuracy: 45.16%\n",
      "Epoch 50/100, Test Loss: 3.2169, Test Accuracy: 36.82%\n",
      "Epoch 51/100, Train Loss: 2.7895, Train Accuracy: 45.27%\n",
      "Epoch 51/100, Test Loss: 3.2050, Test Accuracy: 37.01%\n",
      "Epoch 52/100, Train Loss: 2.7843, Train Accuracy: 45.28%\n",
      "Epoch 52/100, Test Loss: 3.1986, Test Accuracy: 37.17%\n",
      "Epoch 53/100, Train Loss: 2.7776, Train Accuracy: 45.37%\n",
      "Epoch 53/100, Test Loss: 3.1965, Test Accuracy: 37.20%\n",
      "Epoch 54/100, Train Loss: 2.7752, Train Accuracy: 45.41%\n",
      "Epoch 54/100, Test Loss: 3.1738, Test Accuracy: 37.62%\n",
      "Epoch 55/100, Train Loss: 2.7697, Train Accuracy: 45.45%\n",
      "Epoch 55/100, Test Loss: 3.1775, Test Accuracy: 37.36%\n",
      "Epoch 56/100, Train Loss: 2.7646, Train Accuracy: 45.61%\n",
      "Epoch 56/100, Test Loss: 3.1730, Test Accuracy: 37.66%\n",
      "Epoch 57/100, Train Loss: 2.7588, Train Accuracy: 45.64%\n",
      "Epoch 57/100, Test Loss: 3.1652, Test Accuracy: 37.72%\n",
      "Epoch 58/100, Train Loss: 2.7529, Train Accuracy: 45.73%\n",
      "Epoch 58/100, Test Loss: 3.1572, Test Accuracy: 37.65%\n",
      "Epoch 59/100, Train Loss: 2.7502, Train Accuracy: 45.78%\n",
      "Epoch 59/100, Test Loss: 3.1563, Test Accuracy: 37.63%\n",
      "Epoch 60/100, Train Loss: 2.7442, Train Accuracy: 45.87%\n",
      "Epoch 60/100, Test Loss: 3.1519, Test Accuracy: 37.75%\n",
      "Epoch 61/100, Train Loss: 2.7405, Train Accuracy: 45.98%\n",
      "Epoch 61/100, Test Loss: 3.1505, Test Accuracy: 37.72%\n",
      "Epoch 62/100, Train Loss: 2.7366, Train Accuracy: 45.95%\n",
      "Epoch 62/100, Test Loss: 3.1340, Test Accuracy: 38.04%\n",
      "Epoch 63/100, Train Loss: 2.7298, Train Accuracy: 46.11%\n",
      "Epoch 63/100, Test Loss: 3.1258, Test Accuracy: 38.17%\n",
      "Epoch 64/100, Train Loss: 2.7275, Train Accuracy: 46.19%\n",
      "Epoch 64/100, Test Loss: 3.1161, Test Accuracy: 38.42%\n",
      "Epoch 65/100, Train Loss: 2.7224, Train Accuracy: 46.26%\n",
      "Epoch 65/100, Test Loss: 3.1036, Test Accuracy: 38.58%\n",
      "Epoch 66/100, Train Loss: 2.7204, Train Accuracy: 46.27%\n",
      "Epoch 66/100, Test Loss: 3.0998, Test Accuracy: 38.60%\n",
      "Epoch 67/100, Train Loss: 2.7143, Train Accuracy: 46.41%\n",
      "Epoch 67/100, Test Loss: 3.0883, Test Accuracy: 38.74%\n",
      "Epoch 68/100, Train Loss: 2.7118, Train Accuracy: 46.34%\n",
      "Epoch 68/100, Test Loss: 3.0786, Test Accuracy: 38.97%\n",
      "Epoch 69/100, Train Loss: 2.7067, Train Accuracy: 46.53%\n",
      "Epoch 69/100, Test Loss: 3.0758, Test Accuracy: 39.21%\n",
      "Epoch 70/100, Train Loss: 2.7052, Train Accuracy: 46.48%\n",
      "Epoch 70/100, Test Loss: 3.0598, Test Accuracy: 39.37%\n",
      "Epoch 71/100, Train Loss: 2.7009, Train Accuracy: 46.53%\n",
      "Epoch 71/100, Test Loss: 3.0578, Test Accuracy: 39.38%\n",
      "Epoch 72/100, Train Loss: 2.6967, Train Accuracy: 46.60%\n",
      "Epoch 72/100, Test Loss: 3.0395, Test Accuracy: 39.53%\n",
      "Epoch 73/100, Train Loss: 2.6947, Train Accuracy: 46.66%\n",
      "Epoch 73/100, Test Loss: 3.0359, Test Accuracy: 39.70%\n",
      "Epoch 74/100, Train Loss: 2.6906, Train Accuracy: 46.75%\n",
      "Epoch 74/100, Test Loss: 3.0306, Test Accuracy: 39.83%\n",
      "Epoch 75/100, Train Loss: 2.6875, Train Accuracy: 46.79%\n",
      "Epoch 75/100, Test Loss: 3.0239, Test Accuracy: 39.89%\n",
      "Epoch 76/100, Train Loss: 2.6845, Train Accuracy: 46.79%\n",
      "Epoch 76/100, Test Loss: 3.0180, Test Accuracy: 39.97%\n",
      "Epoch 77/100, Train Loss: 2.6856, Train Accuracy: 46.73%\n",
      "Epoch 77/100, Test Loss: 3.0023, Test Accuracy: 40.18%\n",
      "Epoch 78/100, Train Loss: 2.6769, Train Accuracy: 46.97%\n",
      "Epoch 78/100, Test Loss: 2.9800, Test Accuracy: 40.62%\n",
      "Epoch 79/100, Train Loss: 2.6761, Train Accuracy: 46.98%\n",
      "Epoch 79/100, Test Loss: 2.9860, Test Accuracy: 40.45%\n",
      "Epoch 80/100, Train Loss: 2.6720, Train Accuracy: 47.03%\n",
      "Epoch 80/100, Test Loss: 2.9687, Test Accuracy: 40.81%\n",
      "Epoch 81/100, Train Loss: 2.6717, Train Accuracy: 47.00%\n",
      "Epoch 81/100, Test Loss: 2.9716, Test Accuracy: 40.69%\n",
      "Epoch 82/100, Train Loss: 2.6672, Train Accuracy: 47.14%\n",
      "Epoch 82/100, Test Loss: 2.9619, Test Accuracy: 40.77%\n",
      "Epoch 83/100, Train Loss: 2.6641, Train Accuracy: 47.11%\n",
      "Epoch 83/100, Test Loss: 2.9523, Test Accuracy: 40.92%\n",
      "Epoch 84/100, Train Loss: 2.6639, Train Accuracy: 47.15%\n",
      "Epoch 84/100, Test Loss: 2.9555, Test Accuracy: 40.92%\n",
      "Epoch 85/100, Train Loss: 2.6602, Train Accuracy: 47.20%\n",
      "Epoch 85/100, Test Loss: 2.9395, Test Accuracy: 41.04%\n",
      "Epoch 86/100, Train Loss: 2.6576, Train Accuracy: 47.14%\n",
      "Epoch 86/100, Test Loss: 2.9348, Test Accuracy: 41.22%\n",
      "Epoch 87/100, Train Loss: 2.6519, Train Accuracy: 47.34%\n",
      "Epoch 87/100, Test Loss: 2.9241, Test Accuracy: 41.38%\n",
      "Epoch 88/100, Train Loss: 2.6517, Train Accuracy: 47.30%\n",
      "Epoch 88/100, Test Loss: 2.9202, Test Accuracy: 41.51%\n",
      "Epoch 89/100, Train Loss: 2.6501, Train Accuracy: 47.36%\n",
      "Epoch 89/100, Test Loss: 2.9284, Test Accuracy: 41.37%\n",
      "Epoch 90/100, Train Loss: 2.6470, Train Accuracy: 47.36%\n",
      "Epoch 90/100, Test Loss: 2.9151, Test Accuracy: 41.35%\n",
      "Epoch 91/100, Train Loss: 2.6446, Train Accuracy: 47.47%\n",
      "Epoch 91/100, Test Loss: 2.8950, Test Accuracy: 41.86%\n",
      "Epoch 92/100, Train Loss: 2.6406, Train Accuracy: 47.46%\n",
      "Epoch 92/100, Test Loss: 2.8843, Test Accuracy: 41.98%\n",
      "Epoch 93/100, Train Loss: 2.6425, Train Accuracy: 47.45%\n",
      "Epoch 93/100, Test Loss: 2.8854, Test Accuracy: 42.06%\n",
      "Epoch 94/100, Train Loss: 2.6354, Train Accuracy: 47.55%\n",
      "Epoch 94/100, Test Loss: 2.8698, Test Accuracy: 42.21%\n",
      "Epoch 95/100, Train Loss: 2.6330, Train Accuracy: 47.62%\n",
      "Epoch 95/100, Test Loss: 2.8647, Test Accuracy: 42.47%\n",
      "Epoch 96/100, Train Loss: 2.6319, Train Accuracy: 47.65%\n",
      "Epoch 96/100, Test Loss: 2.8559, Test Accuracy: 42.32%\n",
      "Epoch 97/100, Train Loss: 2.6297, Train Accuracy: 47.66%\n",
      "Epoch 97/100, Test Loss: 2.8709, Test Accuracy: 42.21%\n",
      "Epoch 98/100, Train Loss: 2.6254, Train Accuracy: 47.76%\n",
      "Epoch 98/100, Test Loss: 2.8540, Test Accuracy: 42.55%\n",
      "Epoch 99/100, Train Loss: 2.6233, Train Accuracy: 47.82%\n",
      "Epoch 99/100, Test Loss: 2.8636, Test Accuracy: 42.43%\n",
      "Epoch 100/100, Train Loss: 2.6205, Train Accuracy: 47.78%\n",
      "Epoch 100/100, Test Loss: 2.8454, Test Accuracy: 42.77%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Assuming you have separate test sets X_test, y_test already prepared and on device:\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        xb = X[i:i+batch_size].to(device)\n",
    "        yb = y[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)  # Shape: (batch_size, vocab_size) or (batch_size, num_classes)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, dim=1)   # predicted classes\n",
    "        correct += (predicted == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    avg_train_loss = epoch_loss / (len(X) // batch_size)\n",
    "    train_accuracy = correct / total * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # --- Testing / Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            xb = X_test[i:i+batch_size].to(device)\n",
    "            yb = y_test[i:i+batch_size].to(device)\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct_test += (predicted == yb).sum().item()\n",
    "            total_test += yb.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / (len(X_test) // batch_size)\n",
    "    test_accuracy = correct_test / total_test * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c1705f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"next_word.pth\")\n",
    "import pickle\n",
    "with open(\"word2idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2idx, f)\n",
    "with open(\"idx2word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(idx2word, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50368d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, prompt, k=3):\n",
    "    model.eval()\n",
    "    prompt_tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", prompt.lower())\n",
    "    input_seq = prompt_tokens[-seq_length:]\n",
    "    input_ids = [word2idx.get(w, 0) for w in input_seq]\n",
    "    input_tensor = torch.tensor([input_ids]).to(next(model.parameters()).device)  # move to model's device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)                 # logits over vocab\n",
    "        probs = torch.softmax(output, dim=1)         # probabilities\n",
    "        top_probs, top_indices = torch.topk(probs, k)\n",
    "\n",
    "        predictions = [idx2word[idx.item()] for idx in top_indices[0]]\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"welcome to the world of AI!\")    \n",
    "\n",
    "prompt = \"to be or not to be\"\n",
    "predictions = predict_next_word(model, prompt)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"Predicted next words:\", predictions)\n",
    "\n",
    "for i in range(5):\n",
    "    prompt = prompt+\" \"+input(\"Enter a prompt: \")\n",
    "    if prompt.lower() == \"exit\":\n",
    "        break\n",
    "    predictions = predict_next_word(model, prompt)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"Predicted next words:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c3bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
