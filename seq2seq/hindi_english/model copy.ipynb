{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "49c6744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0097a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 0\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "def normalize_sentence(df, lang):\n",
    "    sentence = df[lang].str.lower()\n",
    "    \n",
    "    if lang == 'eng':\n",
    "        sentence = sentence.str.replace(r'[^A-Za-z\\s]+', '', regex=True)\n",
    "        sentence = sentence.str.normalize('NFD')\n",
    "        sentence = sentence.apply(lambda x: x.encode('ascii', errors='ignore').decode('utf-8'))\n",
    "    elif lang == 'hin':\n",
    "        # Keep Devanagari script and basic punctuation\n",
    "        sentence = sentence.str.replace(r'[^\\w\\s\\u0900-\\u097F]', '', regex=True)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentence(df ,lang1, lang2):\n",
    "  sentence1 = normalize_sentence(df, lang1)\n",
    "  sentence2 = normalize_sentence(df, lang2)\n",
    "  return sentence1 , sentence2\n",
    "\n",
    "def read_file(loc,lang1,lang2):\n",
    "  df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2, 'meta'])\n",
    "  return df\n",
    "\n",
    "def process_data(lang1,lang2):\n",
    "  df = read_file('text/%s-%s.txt'%(lang1, lang2), lang1,lang2)\n",
    "  print(f\"Read {len(df)} sentence pairs\")\n",
    "  sentence1, sentence2 = read_sentence(df,lang1,lang2)\n",
    "  source = Lang()\n",
    "  target = Lang()\n",
    "  pairs = []\n",
    "  for i in range(len(df)):\n",
    "    if len(sentence1[i].split()) < MAX_LENGTH and len(sentence2[i].split()) < MAX_LENGTH:\n",
    "      full = [sentence1[i],sentence2[i]]\n",
    "      source.addSentence(sentence1[i])\n",
    "      target.addSentence(sentence2[i])\n",
    "      pairs.append(full)\n",
    "      \n",
    "  return source, target, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "846c5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang,sentence):\n",
    "  return [lang.word2index[word] for word in sentence.split()]\n",
    "\n",
    "def tensorFromSentence(lang,sentence):\n",
    "  indexes = indexesFromSentence(lang,sentence)\n",
    "  indexes.append(EOS_token)\n",
    "  return torch.tensor(indexes,dtype = torch.long,device = device).view(-1,1)\n",
    "\n",
    "def tensorsFromPair(input_lang,output_lang,pair):\n",
    "  input_tensor = tensorFromSentence(input_lang,pair[0])\n",
    "  target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "  return (input_tensor,target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "63fb991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
    "   with torch.no_grad():\n",
    "       input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
    "       output_tensor = tensorFromSentence(output_lang, sentences[1])\n",
    "  \n",
    "       decoded_words = []\n",
    "  \n",
    "       output = model(input_tensor, output_tensor)\n",
    "       # print(output_tensor)\n",
    "  \n",
    "       for ot in range(output.size(0)):\n",
    "           topv, topi = output[ot].topk(1)\n",
    "           # print(topi)\n",
    "\n",
    "           if topi[0].item() == EOS_token:\n",
    "               decoded_words.append('<EOS>')\n",
    "               break\n",
    "           else:\n",
    "               decoded_words.append(output_lang.index2word[topi[0].item()])\n",
    "   return decoded_words\n",
    "\n",
    "def evaluateRandomly(model, source, target, pairs, n=10):\n",
    "   for i in range(n):\n",
    "       pair = random.choice(pairs)\n",
    "       print('source {}'.format(pair[0]))\n",
    "       print('target {}'.format(pair[1]))\n",
    "       output_words = evaluate(model, source, target, pair)\n",
    "       output_sentence = ' '.join(output_words)\n",
    "       print('predicted {}'.format(output_sentence))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd4fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 3061 sentence pairs\n",
      "random sentence ['she gave me a large room while i stayed at her house', 'जब मैं उसके घर पर रही उसने मुझे रहने के लिए एक बड़ा सा कमरा दिया।']\n",
      "Input : 2432 Output : 3129\n",
      "Encoder(\n",
      "  (embedding): Embedding(2432, 256)\n",
      "  (gru): GRU(256, 512)\n",
      ")\n",
      "Decoder(\n",
      "  (embedding): Embedding(3129, 256)\n",
      "  (gru): GRU(256, 512)\n",
      "  (out): Linear(in_features=512, out_features=3129, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "5000 4.3709\n",
      "10000 4.2771\n",
      "15000 4.2573\n",
      "20000 4.2763\n"
     ]
    }
   ],
   "source": [
    "lang1 = 'eng'\n",
    "lang2 = 'hin'\n",
    "source, target, pairs = process_data(lang1, lang2)\n",
    "\n",
    "randomize = random.choice(pairs)\n",
    "print('random sentence {}'.format(randomize))\n",
    "\n",
    "#print number of words\n",
    "input_size = source.n_words\n",
    "output_size = target.n_words\n",
    "print('Input : {} Output : {}'.format(input_size, output_size))\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_iteration = 100000\n",
    "\n",
    "#create encoder-decoder model\n",
    "encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
    "decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "#print model \n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "model = trainModel(model, source, target, pairs, num_iteration)\n",
    "evaluateRandomly(model, source, target, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66f0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
